{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "Implementation of SARSA and Q-Learning on the Frozen Lake game:\n",
    "\n",
    "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. [openai.com](https://gym.openai.com/envs/FrozenLake-v0)\n",
    "\n",
    "| FROZEN                      | L | A | K | E |\n",
    "|-----------------------------|---|---|---|---|\n",
    "| **S:** starting point, safe | S | F | F | F |\n",
    "| **F:** frozen surface, safe | F | H | F | H |\n",
    "|                 **H:** hole | F | F | F | H |\n",
    "|                 **G:** goal | H | F | F | G |\n",
    "\n",
    "- Every step there is a 33% chance to slip and perform a random action instead of the chosen one.\n",
    "- **H** and **G** are terminal states.\n",
    "- Reaching **G** provides a reward of $1$.\n",
    "- Actions are `left`, `down`, `right` and `up`.\n",
    "- Environment is considered solved when reaching an average reward of $0.78$ over 100 consecutive episodes.\n",
    "- [Optimum](https://github.com/openai/gym/blob/37efc3e7d876b7f28c4d675c87137a293c33e2d1/gym/envs/__init__.py#L149-L163) policy would receive a reward of $0.8196$ over 100 consecutive episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['left', 'down', 'right', 'up']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluator():\n",
    "    \"\"\"Pretty print live training stats.\"\"\"\n",
    "    rewards = deque([], 100)\n",
    "    vals = [0]\n",
    "\n",
    "    def stats(episode, reward):\n",
    "        rewards.append(reward)\n",
    "        if len(rewards) == 100:\n",
    "            mean = np.mean(rewards)\n",
    "            if mean >= .78 and vals[0] < .78:\n",
    "                sys.stdout.write(' '.join([''] * 50) + '\\r')\n",
    "                sys.stdout.write('Beat .78 at episode {}.\\n'.format(episode))\n",
    "                sys.stdout.flush()\n",
    "            if mean > vals[0]:\n",
    "                vals[0] = mean\n",
    "                sys.stdout.write('Current 100 epsiode highscore at {}: {:.2f}\\r'\n",
    "                                 .format(episode, mean))\n",
    "                sys.stdout.flush()\n",
    "    return stats\n",
    "\n",
    "\n",
    "def plot_policy(qtable):\n",
    "    \"\"\"Plot the greedy argmax policy values.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title('Policy')\n",
    "    values = qtable.max(axis=1).reshape(4, 4)\n",
    "    policy = qtable.argmax(axis=1).reshape(4, 4)\n",
    "    labels = np.asarray(ACTIONS)[policy]\n",
    "    annot = np.core.defchararray.add(labels, np.char.mod('\\n%.5f', values))\n",
    "    sns.heatmap(values, annot=annot, mask=values == 0, fmt='s', cbar=False)\n",
    "\n",
    "\n",
    "def plot_values(qtable):\n",
    "    \"\"\"Plot all q values in a single big heatmap.\"\"\"\n",
    "    left, down, right, up = np.split(qtable, 4, 1)\n",
    "    qmap = np.zeros((4 * 3, 4 * 3))\n",
    "    qmap[0::3, 1::3] = up.squeeze().reshape(4, 4)\n",
    "    qmap[1::3, 0::3] = left.squeeze().reshape(4, 4)\n",
    "    qmap[1::3, 2::3] = right.squeeze().reshape(4, 4)\n",
    "    qmap[2::3, 1::3] = down.squeeze().reshape(4, 4)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title('Transition values')\n",
    "    annotations = np.char.mod('%.5f', qmap)\n",
    "    mask = qmap == 0\n",
    "    ax = sns.heatmap(qmap, annot=annotations, mask=mask, fmt='s', cbar=False)\n",
    "    plt.hlines([3, 6, 9], *ax.get_xlim())\n",
    "    plt.vlines([3, 6, 9], *ax.get_ylim())\n",
    "    plt.xticks(np.arange(1.5, 12, 3), range(0, 4))\n",
    "    plt.yticks(np.arange(1.5, 12, 3), range(0, 4))\n",
    "\n",
    "\n",
    "# def subplot_values(qtable, action_labels):\n",
    "#     \"\"\"Plot q values in individual subplots per action.\"\"\"\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     for i, actvals in enumerate(np.split(qtable, 4, 1)):\n",
    "#         actvals = actvals.squeeze().reshape(4, 4)\n",
    "#         fig.add_subplot(221 + i, title='Values for: {}'.format(action_labels[i]))\n",
    "#         sns.heatmap(actvals, annot=True, mask=actvals == 0, fmt='.5f', cbar=False)\n",
    "#     return fig\n",
    "\n",
    "\n",
    "def epsilon(episodes, final=.1, initial=1):\n",
    "    \"\"\"Provide a function for linear value annealing.\"\"\"\n",
    "    def anneal(episode):\n",
    "        diff = initial - final\n",
    "        return max([final, initial - (diff * episode / episodes)])\n",
    "\n",
    "    return anneal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen Lake: Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "env.render()\n",
    "terminal = False\n",
    "while not terminal:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, terminal, _ = env.step(action)\n",
    "    print('\\nAction selected: {}\\n'.format(ACTIONS[action]))\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Q-Learning\n",
    "Learning a deterministic environment is always easier. So for this first implementation we turn of the \"*slippyness*\" of the ice. This way each Q value is simply the current reward plus the discounted maximum future reward.\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow r_t + \\gamma * max_{a} Q(s_{t+1},a)$$\n",
    "- Single table of Q values as data structure (states * actions)\n",
    "- $\\epsilon$-greedy policy with linearly decaying $\\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'FrozenLakeDeterministic-v0' not in gym.envs.registry.env_specs:\n",
    "    gym.envs.registration.register(\n",
    "        id='FrozenLakeDeterministic-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=1, # optimum = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLakeDeterministic-v0')\n",
    "\n",
    "episodes = 1000\n",
    "eps = epsilon(episodes / 2, initial=1, final=0)\n",
    "gamma = 0.9\n",
    "\n",
    "qtable = np.zeros((4 * 4, 4))  # ROWS x COLUMNS x ACTIONS\n",
    "stats = evaluator()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        if random.random() < eps(episode):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = qtable[state].argmax()\n",
    "        state_, reward, terminal, _ = env.step(action)\n",
    "        qtable[state, action] = reward + gamma * qtable[state_].max()\n",
    "        state = state_\n",
    "    stats(episode, reward)\n",
    "\n",
    "env.close()\n",
    "plot_policy(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA\n",
    "\n",
    "**S**tate, **A**ction, **R**eward, **S**tate', **A**ction'\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow  Q(s_t,a_t) + \\alpha * [r_t + \\gamma * Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)]$$\n",
    "\n",
    "- Single table of Q values as data structure (states * actions)\n",
    "- $\\epsilon$-greedy policy with linearly decaying $\\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(state, action, reward, state_, action_):\n",
    "    \"\"\"Compute SARSA Q value update.\"\"\"\n",
    "    future_reward = gamma * qtable[state_, action_]\n",
    "    return alpha * (reward + future_reward - qtable[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "episodes = 3000\n",
    "eps = epsilon(episodes / 2, initial=1, final=0)\n",
    "gamma = 0.9\n",
    "alpha = 0.85\n",
    "\n",
    "qtable = np.zeros((4 * 4, 4))  # ROWS x COLUMNS x ACTIONS\n",
    "stats = evaluator()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = np.random.normal(qtable[state], eps(episode)).argmax()\n",
    "        state_, reward, terminal, _ = env.step(action)\n",
    "        action_ = np.random.normal(qtable[state_], eps(episode)).argmax()\n",
    "        qtable[state, action] += sarsa(state, action, reward, state_, action_)\n",
    "        state = state_\n",
    "    stats(episode, reward)\n",
    "\n",
    "env.close()\n",
    "plot_policy(qtable)\n",
    "plot_values(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "Very similar to SARSA, but estimates the Q value according to a exploitation policy instead of the current policy. SARSA learn on-policy: Future Q values are chosen according to the current policy. Q-Learning is greedy: Future Q values are chosen purely by their value.\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha * [r_t + \\gamma * max_a Q(s_{t+1}, a) - Q(s_t,a_t)]$$\n",
    "\n",
    "The reward $r_t$, as in SARSA, is the reward received when taking action $a_t$ in state $s_t$. Sometimes (e.g. on Wikipedia) this reward is denoted as $r_{t+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, action, reward, state_):\n",
    "    \"\"\"Compute Q value update.\"\"\"\n",
    "    future_reward = gamma * qtable[state_].max()\n",
    "    return alpha * (reward + future_reward - qtable[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "episodes = 5000\n",
    "eps = epsilon(episodes / 4, initial=1, final=0)\n",
    "gamma = 0.9\n",
    "alpha = 0.85\n",
    "\n",
    "qtable = np.zeros((4 * 4, 4))  # ROWS x COLUMNS x ACTIONS\n",
    "stats = evaluator()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        action = np.random.normal(qtable[state], eps(episode)).argmax()\n",
    "        state_, reward, terminal, _ = env.step(action)\n",
    "        qtable[state, action] += update(state, action, reward, state_)\n",
    "        state = state_\n",
    "    stats(episode, reward)\n",
    "\n",
    "env.close()\n",
    "plot_policy(qtable)\n",
    "plot_values(qtable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
