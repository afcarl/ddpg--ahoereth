{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deep Reinforcement Learning on Slot Car Racing\n",
    "\n",
    "In this notebook we provide a quick introduction to the \"Deep Deterministic Gradient Policy\" algorithm presented in the 2016 paper \"Continuous Control with Deep Reinforcement Learning\" by Lillicrap et al. We demonstrate the algorithm in a Slot Car Racing (also known by the brand-name Carrera) environment.\n",
    "\n",
    "The following consists of multiple parts:\n",
    "\n",
    "  - The custom `Carrera` environment -- a Python class which allows agents to create, reset and perform \"steps\" on a slot car racing track. It also provides abstractions for visualizing the algorithms performance.\n",
    "  - An `Agent` class which interacts with a provided environment.\n",
    "  - The DDPG algorithm implemented in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Carrera:\n",
    "    \"\"\"A simple carrera track, modeled by a maximum velocity function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create new carrera track environment.\"\"\"\n",
    "        self._track_len = 2 * math.pi\n",
    "        self._position = 0\n",
    "        self._velocity = 0\n",
    "        self._positions = [0]\n",
    "        self._velocities = [0]\n",
    "        self._terminal = False\n",
    "        self._episode_reward = 0\n",
    "        self._max_velocities = np.vectorize(self._max_velocity)\n",
    "        self._fig = None\n",
    "\n",
    "    def _max_velocity(self, position: float):\n",
    "        \"\"\"Returns the maximum velocity for any position on the track.\"\"\"\n",
    "        return (math.sin(position) + 1.2) / 2.2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        self._position = 0\n",
    "        self._velocity = 0\n",
    "        self._terminal = False\n",
    "        self._episode_reward = 0\n",
    "        return (self._position, self._velocity)\n",
    "\n",
    "    def step(self, acceleration=0.) -> Tuple[Tuple[float, float], float, bool]:\n",
    "        \"\"\"Perform a step in the environment.\n",
    "\n",
    "        Returns new observation tuple (position, velocity) and reward.\n",
    "        \"\"\"\n",
    "        acceleration = np.min((1, acceleration))\n",
    "        self._velocity = np.max((0.8 * self._velocity, acceleration))\n",
    "        self._position += self._velocity / 100\n",
    "        self._position %= self._track_len\n",
    "        max_velocity = self._max_velocity(self._position)\n",
    "        if self._velocity > max_velocity:  # Cart flew out of the track\n",
    "            self._terminal = True\n",
    "            return (self._position, self._velocity), -1, self._terminal\n",
    "        reward = ((self._velocity - max_velocity) + 1) * acceleration\n",
    "        self._episode_reward += reward\n",
    "        return (self._position, self._velocity), reward, self._terminal\n",
    "\n",
    "    def render(self) -> plt.Figure:\n",
    "        \"\"\"Render current state.\"\"\"\n",
    "        if self._position < self._positions[-1]:\n",
    "            self._positions = [0]\n",
    "            self._velocities = [0]\n",
    "        if self._fig is None:\n",
    "            self._fig = plt.figure()\n",
    "            x = np.linspace(0, self._track_len, 1000)\n",
    "            y = self._max_velocities(x)\n",
    "            plt.plot(x, y)\n",
    "            self.plot, = plt.plot([0], [0])\n",
    "        self._velocities.append(self._velocity)\n",
    "        self._positions.append(self._position)\n",
    "        self.plot.set_data(self._positions, self._velocities)\n",
    "        self._fig.canvas.draw()\n",
    "        return self._fig\n",
    "\n",
    "    @property\n",
    "    def episode_reward(self) -> float:\n",
    "        \"\"\"Get cummulated reward for the whole episode.\"\"\"\n",
    "        return self._episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Carrera()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "While the paper takes note of prioritized replay methods, it only makes use of sampling experiences uniformly from a limited sized buffer. The straight forward approach for implementing this would be to use `collections.deque`, but sampling from such a queue (as the name maybe already shows...) is [expensive](https://wiki.python.org/moin/TimeComplexity). Therefore we implement a custom memory class which makes use of a basic list and implements the element limit through a pointer which dictates which element is to be overwritten on insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Uniform replay memory with maximum size.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self._buffer = []\n",
    "        self._pointer = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self) < self.max_size:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            self._buffer[self._pointer] = experience\n",
    "            self._pointer = (self._pointer + 1) % self.max_size\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self._buffer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Rather abstract implementation of a reinforcement learning agent. The actual RL model is plug and play, as long as it is implemented consistently.\n",
    "\n",
    "**TODO:** Implement Ornstein-Uhlenbeck noising for exploration. Quote from Lillicrap et al:\n",
    "\n",
    "> For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A reinforcement learning agent.\"\"\"\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    min_memory = 1e3\n",
    "    batchsize = 64\n",
    "\n",
    "    def __init__(self, env, model, render=False, memory_size=1e6):\n",
    "        \"\"\"Create a new reinforcement learning agent.\"\"\"\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.render = render\n",
    "\n",
    "    def train(self, episodes: int):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        total_steps = 0\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            for step in count():\n",
    "                # Perform action, store new experience, train model.\n",
    "                action = self._get_action(state, True)\n",
    "                state_, reward, terminal = self.env.step(action)\n",
    "                self.memory.add((state, action, reward, state_, terminal))\n",
    "                state = state_  # Next state becomes current state.\n",
    "                if self.render and step % 10 == 0:\n",
    "                    env.render()\n",
    "                if len(self.memory) > self.min_memory:\n",
    "                    self.model.train(self.memory.sample(self.batchsize))\n",
    "                if terminal:  # Start new episode if in terminal state.\n",
    "                    break\n",
    "            total_steps += step\n",
    "\n",
    "    def _get_action(self, state, exploration=False) -> Tuple[float]:\n",
    "        action = self.model.get_action(state)\n",
    "        if not exploration:\n",
    "            return action\n",
    "        return action + np.random.normal(0, 0.1, action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Model\n",
    "\n",
    "While we again will create a class for the Deep Deterministic Gradient Policy model, we will implement some of the parts as functions outside of the class in order to better walk through them in this notebook. When implementing this as a script one would want to integrate them all into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import get_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Networks\n",
    "Our model consists of a total of two different networks -- an actor and a critic network. The problem with that approach is that during training we not only optimize those networks, we also use them to dirigate our agent. Manipulating the online policy leads to a feedback loop which leads to instability. While we already use a big memory buffer to mitigate this problem, the authors propose to additionally use two sets of parameters for each network.\n",
    "\n",
    "In the implementation this leads to theoretically four networks, two actor and two critic networks, an online and a target version for each. While the online networks will be used for online predictions and will be updated at every timestep, the target networks will be used for determining the directions in which the online networks should be updated. From time to time the target networks will be updated using the online networks weights -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Networks\n",
    "In order to easily model our networks we use the `namedtuple` collection. Similar to class instances named tuples allow dot-access to their members. For the target networks we only need the network outputs and internal variables, because we will directly manipulate them using the corresponding online network's variables. For the online networks we additionally need a reference to their gradient descent optimizers (for the online training) and direct access to the gradients themself -- more on that in the Actor section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Network = namedtuple('Network', ['y', 'variables'])\n",
    "OptimizableNetwork = namedtuple('Network', ['y', 'variables',\n",
    "                                            'optimizer', 'gradients'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "The critic is the Q value function or Bellman equation approximator. Q values describe the expected reward for an action which normally would be determined through dynamic programming. The critic maps state and action to a single scalar value. This stands in contrast to Deep Q Networks (Mnih et al 2015), where the Q network maps the environment's state to multiple Q values, one for each action. This is because in our case the Q network is not used to determine which action to take, but only to *criticize* whatever action the actor network decided on taking.\n",
    "\n",
    "For the critic network we strickly to the structure described in the paper:\n",
    "\n",
    "  - Two hidden layers with ReLu activation and 400 and 300 neurons respectivley.\n",
    "  - Batch normalization applied to the input and first hidden layer.\n",
    "  - Actions enter the network after the first hidden layer.\n",
    " \n",
    "As common in Deep Q-Networks the single output neuron uses a linear activation.\n",
    "\n",
    "**TODO:** Variable initialization\n",
    "\n",
    "> The final layer weights and biases of both the actor and critic were initialized from a uniform distribution $[−3*10^{−3}, 3*10^{−3}]$ and $[3*10^{−4}, 3*10^{−4}]$ [...]. The other layers were initialized from uniform distributions $[-\\frac{1}{\\sqrt{f}}, \\frac{1}{\\sqrt{f}}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic_network(x, actions, training, name):\n",
    "    \"\"\"Build a critic network q, the value function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        decay = tf.contrib.layers.l2_regularizer(1e-2)\n",
    "        l2 = {'kernel_regularizer': decay, 'bias_regularizer': decay}\n",
    "        norm_0 = tf.layers.batch_normalization(x, training=training)\n",
    "        hidden_1 = tf.layers.dense(norm_0, 400, activation=tf.nn.relu, **l2)\n",
    "        norm_1 = tf.layers.batch_normalization(hidden_1, training=training)\n",
    "        hidden_1_ = tf.concat([norm_1, actions], axis=1)\n",
    "        hidden_2 = tf.layers.dense(hidden_1_, 300, activation=tf.nn.relu, **l2)\n",
    "        y = tf.layers.dense(hidden_2, 1, activation=tf.identity, **l2)\n",
    "        batch_ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        variables = get_variables(scope)\n",
    "    return Network(y, variables), batch_ops\n",
    "\n",
    "\n",
    "def critic(x, actions, targets, training=False):\n",
    "    \"\"\"Build critic online and target network pair.\"\"\"\n",
    "    with tf.variable_scope('critic'):\n",
    "        online, ops = critic_network(x, actions, training, 'online')\n",
    "        target, ops_ = critic_network(x, actions, training, 'target')\n",
    "        loss = tf.reduce_mean(tf.squared_difference(targets, online.y))\n",
    "        gradients = tf.gradients(online.y, actions)\n",
    "        with tf.control_dependencies(ops + ops_):\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    online = OptimizableNetwork(*online, optimizer, gradients)\n",
    "    return online, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor\n",
    "\n",
    "    - Discuss gradient computation/application.\n",
    "\n",
    "**TODO:** Variable initialization\n",
    "\n",
    "> The final layer weights and biases of both the actor and critic were initialized from a uniform distribution $[−3*10^{−3}, 3*10^{−3}]$ and $[3*10^{−4}, 3*10^{−4}]$ [...]. The other layers were initialized from uniform distributions $[-\\frac{1}{\\sqrt{f}}, \\frac{1}{\\sqrt{f}}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_network(x, training, name):\n",
    "    \"\"\"Build an actor network mu, the policy function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        norm_0 = tf.layers.batch_normalization(x, training=training)\n",
    "        hidden_1 = tf.layers.dense(norm_0, 400, activation=tf.nn.relu)\n",
    "        norm_1 = tf.layers.batch_normalization(hidden_1, training=training)\n",
    "        hidden_2 = tf.layers.dense(hidden_1, 300, activation=tf.nn.relu)\n",
    "        norm_2 = tf.layers.batch_normalization(hidden_2, training=training)\n",
    "        y = tf.layers.dense(hidden_2, 1, activation=tf.nn.tanh)\n",
    "        batch_ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        variables = get_variables(scope)\n",
    "    return Network(y, variables), batch_ops\n",
    "\n",
    "\n",
    "def actor(x, critic_gradients, training=False):\n",
    "    \"\"\"Build actor online and target network pair.\"\"\"\n",
    "    with tf.variable_scope('actor'):\n",
    "        online, ops = actor_network(x, training, 'online')\n",
    "        target, ops_ = actor_network(x, training, 'target')\n",
    "        inverse_gradients = tf.multiply(-1., critic_gradients)\n",
    "        gradients = tf.gradients(*online, inverse_gradients)\n",
    "        pairs = zip(gradients, online.variables)\n",
    "        with tf.control_dependencies(ops + ops_):\n",
    "            optimizer = tf.train.AdamOptimizer(1e-4).apply_gradients(pairs)\n",
    "    online = OptimizableNetwork(*online, optimizer, gradients)\n",
    "    return online, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network Updates\n",
    "While the online networks are trained directly (thus the *OptimizableNetwork* name), the target networks are only updated irregularily using the online network's parameters. For this paper describes a process named *soft updates*, which only slowly moves the target network's parameters into the direction of the online network. The original Deep Q- and also the Double Deep Q-Network approach instead just directly copies the parameters over.\n",
    "\n",
    "##### Initial Hard Update\n",
    "In order to ensure the online and target networks initial equallity, we first implement the hard parameter copying. This function will only be used after initial variable initialization to make sure the online and target network start off from the same foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_updates(src: Network, dst: Network):\n",
    "    \"\"\"Overwrite target with online network parameters.\"\"\"\n",
    "    return [target.assign(online)\n",
    "            for online, target in zip(src.variables, dst.variables)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft Update\n",
    "The soft update also consists of the same assign operation as above, but not directly overwrites the target network's parameters but mashes the online and target parameters together. `tau` herein describes how strongly the new values influence the old values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_updates(src: Network, dst: Network, tau=tf.constant(0.001)):\n",
    "    \"\"\"Soft update the dst net's parameters using those of the src net.\"\"\"\n",
    "    return [target.assign(tau * online + (1 - tau) * target)\n",
    "            for online, target in zip(src.variables, dst.variables)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "  - Initializes networks and session.\n",
    "  - Resets TensorFlow graph because notebooks.\n",
    "  - Copies the initial parameters to the target networks.\n",
    "  - Provides `train` function which counts SGD steps.\n",
    "  - Target networks are updated every n SGD steps.\n",
    "  - Provides `get_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\"Deep Deterministic Policy Gradient RL Model.\"\"\"\n",
    "    gamma = 0.99  # Discount factor\n",
    "\n",
    "    def __init__(self, din=2, dout=1):\n",
    "        \"\"\"Create a new DDPG model.\"\"\"\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.states = tf.placeholder(tf.float32, (None, din), 'states')\n",
    "        self.actions = tf.placeholder(tf.float32, (None, dout), 'actions')\n",
    "        self.targets = tf.placeholder(tf.float32, (None,), 'targets')\n",
    "        self.training = tf.placeholder_with_default(False, None, 'training')\n",
    "\n",
    "        self.critic, self.critic_ = critic(self.states, self.actions,\n",
    "                                           self.targets, self.training)\n",
    "        self.actor, self.actor_ = actor(self.states, self.critic.gradients,\n",
    "                                        self.training)\n",
    "\n",
    "        self.soft_updates = soft_updates(self.critic, self.critic_) +\n",
    "                             soft_updates(self.actor, self.actor_)\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.session.run(hard_updates(self.critic, self.critic_) +\n",
    "                         hard_updates(self.actor, self.actor_)\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"Train the online, maybe update the target networks.\n",
    "\n",
    "        NOTE: The whole target computation could be move to TensorFlow by\n",
    "        connecting the target actor outputs directly to the gradients instead\n",
    "        of requesting them from the session and feeding them back in. This\n",
    "        should be implemented, but might require some refactoring. Would\n",
    "        reduce this whole block to a single session run.\n",
    "\n",
    "        NOTE: Currently this completly ignore terminals -- not sure if thats\n",
    "        desired. DQN normally only takes future rewards into consideration\n",
    "        for states which are not terminal states. Lillicrap et al do not\n",
    "        make this distinction.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, states_, terminals=zip(*batch)\n",
    "        actions_=self.session.run(self.actor_.y, {self.states: states_})\n",
    "        q_values=self.session.run(self.critic_.y, {self.states: states_,\n",
    "                                                     self.actions: actions_})\n",
    "        q_values=np.squeeze(q_values)\n",
    "        targets=rewards + self.gamma * q_values\n",
    "        # targets = rewards + self.gamma * q_values * np.invert(terminals)  #\n",
    "        # DQN\n",
    "        self.session.run([self.critic.optimizer, self.actor.optimizer],\n",
    "                         {self.states: states, self.targets: targets,\n",
    "                          self.actions: actions, self.training: True})\n",
    "\n",
    "        # Update target networks.\n",
    "        self.session.run(self.soft_updates)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action, = self.session.run(self.actor.y, {self.states: [state]})\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Carrera()\n",
    "model = DDPG()\n",
    "agent = Agent(env, model, render=True)\n",
    "agent.train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
