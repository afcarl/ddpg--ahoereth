{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deep Reinforcement Learning on Slot Car Racing\n",
    "\n",
    "In this notebook we provide a quick introduction to the \"Deep Deterministic Gradient Policy\" algorithm presented in the 2016 paper \"Continuous Control with Deep Reinforcement Learning\" by Lillicrap et al. We demonstrate the algorithm in a Slot Car Racing (also known by the brand-name Carrera) environment.\n",
    "\n",
    "The following consists of multiple parts:\n",
    "\n",
    "  - The custom `Carrera` environment -- a Python class which allows agents to create, reset and perform \"steps\" on a slot car racing track. It also provides abstractions for visualizing the algorithms performance.\n",
    "  - An `Agent` class which interacts with a provided environment.\n",
    "  - The DDPG algorithm implemented in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Carrera:\n",
    "    \"\"\"A simple carrera track, modeled by a maximum velocity function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Create new carrera track environment.\"\"\"\n",
    "        self._track_len = 2 * math.pi\n",
    "        self._position = 0\n",
    "        self._velocity = 0\n",
    "        self._terminal = False\n",
    "        self._episode_reward = 0\n",
    "\n",
    "    def _max_velocity(self, position: float):\n",
    "        \"\"\"Returns the maximum velocity for any position on the track.\"\"\"\n",
    "        return (math.sin(position) + 1) / 2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        self._position = 0\n",
    "        self._velocity = 0\n",
    "        self._terminal = False\n",
    "        self._episode_reward = 0\n",
    "        return (self._position, self._velocity)\n",
    "\n",
    "    def step(self, acceleration=0.) -> Tuple[Tuple[float, float], float, bool]:\n",
    "        \"\"\"Perform a step in the environment.\n",
    "\n",
    "        Returns new observation tuple (position, velocity) and reward.\n",
    "        \"\"\"\n",
    "        self._velocity = np.max((0.8 * self._velocity, np.min((1, acceleration))))\n",
    "        self._position = (self._position + (self._velocity /\n",
    "                                            self._track_len)) % self._track_len\n",
    "        max_velocity = self._max_velocity(self._position)\n",
    "        if self._velocity > max_velocity:  # Cart flew out of the track\n",
    "            self._terminal = True\n",
    "            return (self._position, self._velocity), -1, self._terminal\n",
    "        reward = (self._velocity - max_velocity) + 1\n",
    "        self._episode_reward += reward\n",
    "        return (self._position, self._velocity), reward, self._terminal\n",
    "\n",
    "    def render(self, fig=None) -> plt.Figure:\n",
    "        \"\"\"Render current state.\"\"\"\n",
    "        if fig is None:\n",
    "            fig = plt.figure()\n",
    "        x = np.linspace(0, self._track_len, 1000)\n",
    "        y = np.vectorize(self._max_velocity)(x)\n",
    "        plt.plot(x, y)\n",
    "        fig.canvas.draw()\n",
    "        return fig\n",
    "\n",
    "    @property\n",
    "    def episode_reward(self) -> float:\n",
    "        \"\"\"Get cummulated reward for the whole episode.\"\"\"\n",
    "        return self._episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Carrera()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "While the paper takes note of prioritized replay methods, it only makes use of samplying experiences uniformly from a limited sized buffer. The straight forward approach for implementing this would be to use `collections.deque`, sampling from such a queue (as the name says...) is [expensive](https://wiki.python.org/moin/TimeComplexity). Therefore we implement a custom memory class which makes use of a basic list and implements the element limit through a pointer which dictates which element to overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Uniform replay memory with maximum size.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self._buffer = []\n",
    "        self._pointer = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self) < self.max_size:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            self._buffer[self._pointer] = experience\n",
    "            self._pointer = (self._pointer + 1) % self.max_size\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self._buffer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Rather abstract implementation of a reinforcement learning agent. The actual RL model is plug and play, as long as it is implemented consistently.\n",
    "\n",
    "**TODO:** Implement Ornstein-Uhlenbeck noising for exploration. Quote from Lillicrap et al:\n",
    "\n",
    "> For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A reinforcement learning agent.\"\"\"\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    min_memory = 1e3\n",
    "\n",
    "    def __init__(self, env, model, memory_size=1e6):\n",
    "        \"\"\"Create a new reinforcement learning agent.\"\"\"\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.noise = .1\n",
    "\n",
    "    def train(self, episodes: int):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        total_steps = 0\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            for step in count():\n",
    "                # Perform action, store new experience, train model.\n",
    "                action = self._get_action(state, True)\n",
    "                state_, reward, terminal = self.env.step(action)\n",
    "                self.memory.add((state, action, reward, state_, terminal))\n",
    "                state = state_  # Next state becomes current state.\n",
    "                if len(self.memory) > self.min_memory:\n",
    "                    self.model.train(self.memory.sample(64))\n",
    "                if terminal:  # Start new episode if in terminal state.\n",
    "                    break\n",
    "            total_steps += step\n",
    "\n",
    "    def _get_action(self, state, exploration=False):\n",
    "        action = self.model.get_action(state)\n",
    "        if not exploration:\n",
    "            return action\n",
    "        return action + np.random.normal(0, self.sigma, action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Model\n",
    "\n",
    "While we again will create a class for the Deep Deterministic Gradient Policy model, we will implement some of the parts as functions outside of the class in order to better walk through them in this notebook. When implementing this as a script one would want to integrate them all into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Networks\n",
    "Our model consists of a total of two different networks -- an actor and a critic network. The problem with that approach is that during training we not only optimize those networks, we also use them to dirigate our agent. Manipulating the online policy leads to a feedback loop which leads to instability. While we already use a big memory buffer to mitigate this problem, the authors propose to additionally use two sets of parameters for each network.\n",
    "\n",
    "In the implementation this leads to theoretically four networks, two actor and two critic networks, an online and a target version for each. While the online networks will be used for online predictions and will be updated at every timestep, the target networks will be used for determining the directions in which the online networks should be updated. From time to time the target networks will be updated using the online networks weights -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Networks\n",
    "In order to easily model our networks we use the `namedtuple` collection. Similar to class instances named tuples allow dot-access to their members. For the target networks we only need the network outputs and internal variables, because we will directly manipulate them using the corresponding online network's variables. For the online networks we additionally need a reference to their gradient descent optimizers (for the online training) and direct access to the gradients themself -- more on that in the Actor section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Network(NamedTuple):\n",
    "    y: tf.Tensor\n",
    "    variables: List[tf.Tensor]\n",
    "\n",
    "\n",
    "class OptimizableNetwork(NamedTuple, Network):\n",
    "    y: tf.Tensor\n",
    "    variables: List[tf.Tensor]\n",
    "    optimizer: tf.Operation\n",
    "    gradients: List[tf.Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "The critic is the Q value function or Bellman equation approximator. Q values describe the expected reward for an action which normally would be determined through dynamic programming. The critic maps state and action to a single scalar value. This stands in contrast to Deep Q Networks (Mnih et al 2015), where the Q network maps the environment's state to multiple Q values, one for each action. This is because in our case the Q network is not used to determine which action to take, but only to *criticize* whatever action the actor network decided on taking.\n",
    "\n",
    "For the critic network we strickly to the structure described in the paper:\n",
    "\n",
    "  - Two hidden layers with ReLu activation and 400 and 300 neurons respectivley.\n",
    "  - Batch normalization applied to the input and first hidden layer.\n",
    "  - Actions enter the network after the first hidden layer.\n",
    " \n",
    "As common in Deep Q-Networks the single output neuron uses a linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic_network(states, actions, name):\n",
    "    \"\"\"Build a critic network q, the value function approximator.\n",
    "\n",
    "    TODO: L2 weight decay with 1e-2\n",
    "    TODO: Batchnorm\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        hidden_1 = tf.layers.dense(states, 400, activation=tf.nn.relu)\n",
    "        hidden_1_ = tf.concat([hidden_1, actions], axis=1)\n",
    "        hidden_2 = tf.layers.dense(hidden_1_, 300, activation=tf.nn.relu)\n",
    "        outputs = tf.layers.dense(hidden_2, 1, activation=tf.identity)\n",
    "        variables = tf.contrib.framework.get_variables(scope)\n",
    "    return Network(outputs, variables)\n",
    "\n",
    "\n",
    "def critic(states, actions, targets):\n",
    "    \"\"\"Build critic online and target network pair.\"\"\"\n",
    "    with tf.variable_scope('critic'):\n",
    "        target = critic_network(states, actions, 'target')\n",
    "        outputs, variables = critic_network(states, actions, 'online')\n",
    "        loss = tf.reduce_mean(tf.squared_difference(targets, outputs))\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "        gradients = tf.gradients(outputs, actions)\n",
    "    online = OptimizableNetwork(outputs, variables, optimizer, gradients)\n",
    "    return online, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_network(states, name):\n",
    "    \"\"\"Build an actor network mu, the policy function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        hidden_1 = tf.layers.dense(states, 400, activation=tf.nn.relu)\n",
    "        hidden_2 = tf.layers.dense(hidden_1, 300, activation=tf.nn.relu)\n",
    "        outputs = tf.layers.dense(hidden_2, 1, activation=tf.nn.tanh)\n",
    "        variables = tf.contrib.framework.get_variables(scope)\n",
    "    return Network(outputs, variables)\n",
    "\n",
    "\n",
    "def actor(states, critic_gradients):\n",
    "    \"\"\"Build actor online and target network pair.\"\"\"\n",
    "    with tf.variable_scope('actor'):\n",
    "        target = actor_network(states, 'target')\n",
    "        outputs, variables = actor_network(states, 'online')\n",
    "        inverse_gradients = tf.multiply(-1., critic_gradients)\n",
    "        gradients = tf.gradients(outputs, variables, inverse_gradients)\n",
    "        pairs = zip(gradients, variables)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4).apply_gradients(pairs)\n",
    "    online = OptimizableNetwork(outputs, variables, optimizer, gradients)\n",
    "    return online, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network Updates\n",
    "While the online networks are trained directly (thus the *OptimizableNetwork* name), the target networks are only updated irregularily using the online network's parameters. For this paper describes a process named *soft updates*, which only slowly moves the target network's parameters into the direction of the online network. The original Deep Q- and also the Double Deep Q-Network approach instead just directly copies the parameters over.\n",
    "\n",
    "##### Initial Hard Update\n",
    "In order to ensure the online and target networks initial equallity we first implement the hard parameter copying, `copy_network_parameters`. This function will only be used after initial variable initialization to make sure the online and target network start off from the same foundation. The fun part here is, that we can do this directly in TensorFlow by directly assigning one Tensor to another. The function therefore just consists of two lines of code, one to match the pairs together and one to call the session with the assign operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_network_parameters(session, src: Network, dst: Network):\n",
    "    \"\"\"Overwrite target with online network parameters.\"\"\"\n",
    "    pairs = zip(src.variables, dst.variables)\n",
    "    session.run([dst.assign(src) for src, dst in pairs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft Update\n",
    "The soft update also consists of the assign operation, but we first need to mash the parameters together using the `soft_update` function. `tau` herein describes how strongly the new values influence the previous target values. Note the notation: From now on the trailing underscore will describe variables related to either one of our target networks, similar to the pipe ($Q'$ or $mu'$) used in the paper.\n",
    "\n",
    "Because we actually need to manipulate the variables' values, the `update_network_parameters` function gets a bit more complex. It first needs to fetch the tensors' current values from the TensorFlow session, then match the correct triplets of `(target_tensor, online_value, target_value)` together and assign the result of our `soft_update` function to the target tensor back in the TensorFlow session.\n",
    "\n",
    "A possible enhancement here would be to move the soft update value calculation to the TensorFlow graph itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_update(val, val_, tau=0.001):\n",
    "    \"\"\"Calculate the soft update values.\"\"\"\n",
    "    return tau * val + (1 - tau) * val_\n",
    "\n",
    "\n",
    "def update_network_parameters(session, src: Network, dst: Network, tau=0.001):\n",
    "    \"\"\"Soft update the dst net's parameters using those of the src net.\"\"\"\n",
    "    count = len(src.variables)\n",
    "    values = session.run(src.variables + dst.variables)\n",
    "    triplets = zip(dst.variables, values[:count], values[count:])\n",
    "    session.run([tensor.assign(soft_update(val, val_, tau))\n",
    "                 for tensor, val, val_ in triplets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "  - Initializes networks and session.\n",
    "  - Resets TensorFlow graph because notebooks.\n",
    "  - Copies the initial parameters to the target networks.\n",
    "  - Provides `train` function which counts SGD steps.\n",
    "  - Target networks are updated every n SGD steps.\n",
    "  - Provides `get_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    gamma = 0.99  # Discount factor\n",
    "    update_frequency = 100\n",
    "\n",
    "    def __init__(self, din=2, dout=1):\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.states = tf.placeholder(tf.float32, (None, din), 'states')\n",
    "        self.actions = tf.placeholder(tf.float32, (None, dout), 'actions')\n",
    "        self.targets = tf.placeholder(tf.float32, (None,), 'targets')\n",
    "\n",
    "        self.critics = critic(self.states, self.actions, self.targets)\n",
    "        self.critic, self.critic_ = self.critics\n",
    "\n",
    "        self.actors = actor(self.states, self.critic.gradients)\n",
    "        self.actor, self.actor_ = self.actors\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        copy_network_parameters(self.session, self.critic, self.critic_)\n",
    "        copy_network_parameters(self.session, self.actor, self.actor_)\n",
    "\n",
    "        self.update_count = 0\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"Train the online, maybe update the target networks.\n",
    "\n",
    "        NOTE: The whole target computation could be move to TensorFlow by \n",
    "        connecting the target actor outputs directly to the gradients instead\n",
    "        of requesting them from the session and feeding them back in. This\n",
    "        should be implemented, but might require some refactoring. Would\n",
    "        reduce this whole block to a single session run.\n",
    "\n",
    "        NOTE: Currently this completly ignore terminals -- not sure if thats\n",
    "        desired. DQN normally only takes future rewards into consideration\n",
    "        for states which are not terminal states. Lillicrap et al do not\n",
    "        make this distinction.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, states_, _ = zip(*batch)\n",
    "        actions_ = self.session.run(self.actor_.y, {self.states: states_})\n",
    "        q_values = self.session.run(self.critic_.y, {self.states: states_,\n",
    "                                                     self.actions: actions_})\n",
    "        targets = rewards + self.gamma * np.squeeze(q_values)\n",
    "        # targets = rewards + self.gamma * ys * np.invert(terminals)  # DQN\n",
    "        self.session.run([self.critic.optimizer, self.actor.optimizer],\n",
    "                         {self.states: states, self.targets: targets,\n",
    "                          self.actions: actions})\n",
    "\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.update_frequency == 0:\n",
    "            update_network_parameters(self.session, self.critic, self.critic_)\n",
    "            update_network_parameters(self.session, self.actor, self.actor_)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action, = self.session.run(self.actor.y, {self.states: [state]})\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Carrera()\n",
    "model = DDPG()\n",
    "agent = Agent(env, model)\n",
    "agent.train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
