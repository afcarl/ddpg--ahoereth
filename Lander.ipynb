{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deep Reinforcement Learning\n",
    "\n",
    "In this notebook we provide an introduction to the \"Deep Deterministic Policy Gradient\" algorithm presented in the 2016 paper \"Continuous Control with Deep Reinforcement Learning\" by Lillicrap et al. The following implementation is ready to run and will guide through all steps from using an OpenAI Gym environment over interacting with it and storing experiences in a memory buffer to creating all required parts of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Lunar Lander\n",
    "\n",
    "As the task on hand we will consider the OpenAI Continuous Lunar Lander environment. It is structured as follows:\n",
    "\n",
    "> The Landing pad is always at coordinates $(0, 0)$. Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about $100..140$ points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is $+10$. Firing main engine is $-0.3$ points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from $-1$ to $+1$. First controls main engine, $-1..0$ off, $0..+1$ throttle from $50\\%$ to $100\\%$ power. Engine can't work with less than $50\\%$ power. Second value $-1.0..-0.5$ fire left engine, $+0.5..+1.0$ fire right engine, $-0.5..0.5$ off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "state = env.reset()\n",
    "terminal = False\n",
    "while not terminal:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, terminal, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "While the paper takes note of prioritized replay methods, it only makes use of sampling experiences uniformly from a limited sized buffer. The straight forward approach for implementing this would be to use `collections.deque`, but sampling from such a queue (as the name maybe already shows...) is [expensive](https://wiki.python.org/moin/TimeComplexity). Therefore we implement a custom memory class which makes use of a basic list and implements the element limit through a pointer which dictates which element is to be overwritten on insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Uniform replay memory with maximum size.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity=1e6):\n",
    "        \"\"\"Create a new replay memory.\"\"\"\n",
    "        self.capacity = capacity\n",
    "        self._buffer = []\n",
    "        self._pointer = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get amount of currently stored experiences.\"\"\"\n",
    "        return len(self._buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\"Add experience to replay buffer.\"\"\"\n",
    "        if len(self) < self.capacity:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            self._buffer[self._pointer] = experience\n",
    "            self._pointer = (self._pointer + 1) % self.capacity\n",
    "\n",
    "    def sample(self, n):\n",
    "        \"\"\"Sample from memory.\"\"\"\n",
    "        return random.sample(self._buffer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Rather abstract implementation of a reinforcement learning agent. The actual RL model is plug and play, as long as it is implemented consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A reinforcement learning agent.\"\"\"\n",
    "\n",
    "    eps_min = .1\n",
    "\n",
    "    def __init__(self, env, memory, model):\n",
    "        \"\"\"Create a new reinforcement learning agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.memory = memory\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, episodes):\n",
    "        \"\"\"Play many episodes with many steps.\n",
    "\n",
    "        Stores observations in memory, starts the feed threads and calls\n",
    "        the model's training function when memory is big enough.\n",
    "        \"\"\"\n",
    "        eps = 1\n",
    "        eps_rate = 1 / (episodes / 2)\n",
    "\n",
    "        stats = []\n",
    "        total_steps = 0\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "\n",
    "            terminal = False\n",
    "            state = self.env.reset()\n",
    "            while not terminal:\n",
    "                # Perform action & store experience.\n",
    "                action = self.model.get_action(state, eps)\n",
    "                state_, reward, terminal, _ = self.env.step(action)\n",
    "                self.memory.add((state, action, reward, state_, terminal))\n",
    "                state = state_  # Next state becomes current state.\n",
    "\n",
    "                # Train model.\n",
    "                self.model.train()\n",
    "\n",
    "                # Accumulate reward for progress stats & maybe restart episode.\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "\n",
    "            stats.append((episode_reward, episode_steps))\n",
    "            total_steps += episode_steps\n",
    "\n",
    "            # Decay epsilon (maybe remove this).\n",
    "            if eps > self.eps_min:\n",
    "                eps -= eps_rate\n",
    "\n",
    "            # Print stats from time to time.\n",
    "            if episode % 10 == 0:\n",
    "                stats = np.asarray(stats)\n",
    "                rargmax, _ = stats.argmax(0)\n",
    "                print('Episode {}, max reward/steps {:.2f}/{:.2f}, '\n",
    "                      'average reward/steps {:.2f}/{:.2f}'\n",
    "                      .format(episode, *stats[rargmax], *stats.mean(0)))\n",
    "                print('total_steps', total_steps)\n",
    "                stats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Model\n",
    "\n",
    "While we again will create a class for the Deep Deterministic Gradient Policy model, we will implement some of the parts as functions outside of the class in order to better walk through them in this notebook. When implementing this as a script one would want to integrate them all into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import get_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Networks\n",
    "Our model consists of a total of two different networks -- an actor and a critic network. The problem with that approach is that during training we not only optimize those networks, we also use them to dirigate our agent. Manipulating the online policy leads to a feedback loop which leads to instability. While we already use a big memory buffer to mitigate this problem, the authors propose to additionally use two sets of parameters for each network.\n",
    "\n",
    "In the implementation this leads to theoretically four networks, two actor and two critic networks, an online and a target version for each. While the online networks will be used for online predictions and will be updated at every timestep, the target networks will be used for determining the directions in which the online networks should be updated. From time to time the target networks will be updated using the online networks weights -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Networks\n",
    "In order to easily model all the networks which we need to create, we use the `namedtuple` collection. Similar to class instances named tuples allow dot-access to their members. Each network has some output `y`, some variables `var` and some operations `ops`. The advantage of such an approach is that members of the graph are more consistently accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Network = namedtuple('Network', ['y', 'vars', 'ops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layers\n",
    "Lillicrap et al 2016 describes very specific dense layers:\n",
    "\n",
    "- Weights and biases are initialized from a bounded uniform distribution where the bounds depend on the previous layer's size (or are fixed for output layers).\n",
    "- All critic layers use l2 weight decay.\n",
    "\n",
    "In order to not repeat all the initialization code we predefine a wrapper function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, units, activation=tf.identity, decay=None, minmax=None):\n",
    "    \"\"\"Build a dense layer with uniform initialization and optional loss.\"\"\"\n",
    "    if minmax is None:\n",
    "        minmax = float(x.shape[1].value) ** -.5\n",
    "    return tf.layers.dense(\n",
    "        x,\n",
    "        units,\n",
    "        activation=activation,\n",
    "        kernel_initializer=tf.random_uniform_initializer(-minmax, minmax),\n",
    "        bias_initializer=tf.random_uniform_initializer(-minmax, minmax),\n",
    "        kernel_regularizer=decay and tf.contrib.layers.l2_regularizer(1e-3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "The critic is the value function, you can also think about this as the Bellman equation approximator. It returns Q values which describe the expected reward for an action. Those values would normally be determined through dynamic programming. The critic maps a state/action pair to a single scalar value. This stands in contrast to Deep Q Networks (Mnih et al 2015), where the Q network maps the environment's state to a vector of Q values, one for each action. This is because in our case the Q network is not used to determine which action to take (DQN uses a deterministic `argmax` policy on the Q value vector), but only to *criticize* whatever action the actor network decides on taking.\n",
    "\n",
    "We strictly stick to the network structure described in the paper:\n",
    "\n",
    "  - Two hidden layers with ReLu activation and 400 and 300 neurons respectivley.\n",
    "  - <s>Batch normalization applied to the input and first hidden layer.</s><br>*[Removed](https://github.com/ahoereth/ddpg/blob/https://github.com/ahoereth/ddpg/blob/ea02a94/Lander.ipynb) batch normalization  because it let to massive instability.*\n",
    "  - Actions enter the network after the first hidden layer.\n",
    " \n",
    "As common in Deep Q-Networks the single output neuron uses a linear activation. The `critic` function below is designed such that we can reuse it for both the online and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critic(x, actions, name='online', reuse=False):\n",
    "    \"\"\"Build a critic network q, the value function approximator.\"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "        training = tf.shape(x)[0] > 1  # Currently training if passed a batch.\n",
    "        net = tf.layers.batch_normalization(x, training=training)\n",
    "        net = dense(net, 400, tf.nn.relu, decay=True)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = tf.concat([net, actions], axis=1)\n",
    "        net = dense(net, 300, tf.nn.relu, decay=True)\n",
    "        y = dense(net, 1, decay=True, minmax=3e-4)\n",
    "        ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        return Network(tf.squeeze(y), get_variables(scope), ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critic is simply optimized by minimizing the mean squared error between the Q target values which are computed using the Bellman approximation on the experiences we make in the environment (we will do so below) and the actual output from the network. In the following cell we also bind the operations of the online critic (to update the running averages) to the minimize operation -- this way they will be called everytime we call the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(critic, critic_, terminals, rewards, gamma=.99):\n",
    "    \"\"\"Build critic network optimizer minimizing MSE.\"\"\"\n",
    "    with tf.variable_scope('critic'):\n",
    "        targets = tf.where(terminals, rewards, rewards + gamma * critic_.y)\n",
    "        mse = tf.reduce_mean(tf.squared_difference(targets, critic.y))\n",
    "        tf.summary.scalar('loss', mse)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        with tf.control_dependencies(critic.ops):\n",
    "            return optimizer.minimize(mse, tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor\n",
    "Actor networks describe the policy the agent should follow in any given state. Given some input the policy deterministically provides an action. Actions here are vectors of real values -- in a racing environment such a vector would for example contain a steering angle and a acceleration value.\n",
    "\n",
    "The actor networks have a simillar structure as the critic, but do not receive the actions as input at any point. Further more there is no weight decay applied and the output uses a hyperbolic tangent activation function to bound the actions between $-1$ and $1$ -- which needs to be tweaked for different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_actor(x, dout, max_out, name='online'):\n",
    "    \"\"\"Build an actor network mu, the policy function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        training = tf.shape(x)[0] > 1  # Currently training if passed a batch.\n",
    "        net = tf.layers.batch_normalization(x, training=training)\n",
    "        net = dense(net, 400, tf.nn.relu)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "        net = dense(net, 300, tf.nn.relu)\n",
    "        net = tf.layers.batch_normalization(net, training=training)\n",
    "        y = dense(net, dout, tf.nn.tanh, minmax=3e-4)\n",
    "        scaled = y * max_out\n",
    "        ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        return Network(scaled, get_variables(scope), ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Gradients\n",
    "Our goal is to tweak the critic network's parameters such that it outputs action values, which, as input to the critic network, result in good results (*aka* high Q values *aka* profit): The actor is trained by ascending the gradients of the critic with respect to the actor's actions.\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta_{\\theta^\\mu}J \\approx& \\Delta_{\\theta^\\mu} Q(s_t, \\mu(s_t|\\theta^\\mu)|\\theta^Q) & \\\\\n",
    "=& \\Delta_a\\ \\,Q(s_t, \\mu(s_t|\\theta^\\mu)|\\theta^Q) \\Delta_{\\theta^\\mu} \\mu(s_t|\\theta^\\mu) & F'(x) = f'(g(x))g'(x)\n",
    "\\end{align}\n",
    "\n",
    "To do so, we need to perform gradient **ascent** on the critic's gradient with respect to the actor's action output (which is an input to the critic, a necessary condition for computing such gradient). That gradient, lets call it *value gradient*, depicts the direction in which to adjust the action (originally the output of the actor network) to minimize the Q values -- we ascent it, because we want to maximize the Q values.\n",
    "\n",
    "$$\\Delta_a Q(s,\\mu(s_t|\\theta^\\mu)|\\theta^Q)$$\n",
    "\n",
    "The next step is to take those gradients to the **actor** network, although they originally came from the **critic** network. Normally TensorFlow would worry about optimizing the network for us by minimizing some scalar loss we provide. The magic in this is, that one does not actually pass a scalar to the optimizer's `minimize` function, but a Tensor node, which depends on a complete graph of computations -- those are just commonly abstracted away. When aiming to minimize such a loss, TensorFlow visits all the nodes on which that final loss node depends and internally computes the gradients to modulate them a little bit such that the loss decays.\n",
    "\n",
    "Our problem is: We do not have such a loss node, but a complete set of gradients from the critic network. We now need to manually modulate the actor network's gradients, such that they move a little bit into the uphill direction of the critic network's gradients.\n",
    "\n",
    "$$\\Delta_a Q(s_t, \\mu(s_t|\\theta^\\mu)|\\theta^Q) \\Delta_{\\theta^\\mu} \\mu(s_t|\\theta^\\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(actor, critic):\n",
    "    \"\"\"Build actor network optimizier performing action gradient ascent.\"\"\"\n",
    "    with tf.variable_scope('actor'):\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        # What is `actor.y`'s influence on the critic network's output?\n",
    "        value_gradient, = tf.gradients(critic.y, actor.y)  # (batchsize, dout)\n",
    "        # Use `value_gradient` as initial value for the `actor.y` gradients --\n",
    "        # normally this is set to 1s by TF. Results in a value per parameter.\n",
    "        policy_gradients = tf.gradients(actor.y, actor.vars, -value_gradient)\n",
    "        gradient_pairs = zip(policy_gradients, actor.vars)\n",
    "        with tf.control_dependencies(actor.ops):\n",
    "            return optimizer.apply_gradients(gradient_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network Updates\n",
    "While the online networks are trained directly (thus the *OptimizableNetwork* name), the target networks are only updated irregularily using the online network's parameters. For this paper describes a process named *soft updates*, which only slowly moves the target network's parameters into the direction of the online network. The original Deep Q- and also the Double Deep Q-Network approach instead just directly copies the parameters over.\n",
    "\n",
    "##### Initial Hard Update\n",
    "In order to ensure the online and target networks initial equallity, we first implement the hard parameter copying. This function will only be used after initial variable initialization to make sure the online and target network start off from the same foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_updates(src, dst):\n",
    "    \"\"\"Overwrite target with online network parameters.\"\"\"\n",
    "    with tf.variable_scope('hardupdates'):\n",
    "        return [target.assign(online)\n",
    "                for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft Update\n",
    "The soft update also consists of the same assign operation as above, but not directly overwrites the target network's parameters but mashes the online and target parameters together. `tau` herein describes how strongly the new values influence the old values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_updates(src: Network, dst: Network, tau=1e-3):\n",
    "    \"\"\"Soft update the dst net's parameters using those of the src net.\"\"\"\n",
    "    with tf.variable_scope('softupdates'):\n",
    "        return [target.assign(tau * online + (1 - tau) * target)\n",
    "                for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise\n",
    "\n",
    "**TODO:** Explain Ornstein-Uhlenbeck process noise and RL exploration strategies.\n",
    "\n",
    "Quote from Lillicrap et al:\n",
    "\n",
    "> For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n, theta=.15, sigma=.2):\n",
    "    \"\"\"Ornstein-Uhlenbeck noise process.\"\"\"\n",
    "    with tf.variable_scope('OUNoise'):\n",
    "        state = tf.Variable(tf.zeros((n,)))\n",
    "        noise = -theta * state + sigma * tf.random_normal((n,))\n",
    "        return state.assign_add(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together: The DDPG-Model Class\n",
    "The following class mainly makes use of all the functions defined above to create the rather complex TensorFlow graph -- that primarily happens in the `__init__` function: Stiching together TensorFlow input placeholders with the 4 networks and stiching those networks together with one another.\n",
    "\n",
    "#### Input Pipeline\n",
    "The other major chunk of code from the class goes into the data pipeline: Instead of using the very common but [officially discouraged](https://github.com/tensorflow/tensorflow/issues/2919) `feed_dict` approach, we make use of a `tf.FIFOQueue` for holding a steady amount of samples from the memory inside the TensorFlow graph environment and ready to be consumed by our complex training step. \n",
    "\n",
    "The queue is fed from threads running independent of our main environment interaction. Similarily training happens in its own threads. The combination of those two approaches massivley speeds up the whole reinforcement learning process because the main bottleneck is creating new samples (from interacting with the environment) -- that process can now consume the complete main thread without being significantly slowed down by all the training complexitiy.\n",
    "\n",
    "TensorFlow hereby worries about most of the threading complexity: Feeding the queue automatically blocks when the queue is at its capacity and the trainers automatically wait when the queue is empty. The trick here is to never let the latter happen! We automatically start as many threads as possible in order to perform as many training steps as expected by the agent and we automatically start as many feeding threads as required to not starve the trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\"Deep Deterministic Policy Gradient RL Model.\"\"\"\n",
    "\n",
    "    batchsize = 64\n",
    "\n",
    "    def __init__(self, din=2, dout=1, action_bounds=1, memory=None, checkpoint=None):\n",
    "        \"\"\"Create a new DDPG model.\"\"\"\n",
    "        self.bounds = action_bounds\n",
    "        self.memory = memory\n",
    "        self.train_queue = Queue()\n",
    "        self._train_threads = []\n",
    "        self._feed_threads = []\n",
    "\n",
    "        # Reset graph and recreate it.\n",
    "        tf.reset_default_graph()\n",
    "        tf.train.create_global_step()\n",
    "        self.global_step = tf.train.get_global_step()\n",
    "        self.session = tf.Session()\n",
    "\n",
    "        # The queue is fed samples from the replay memory in an independent\n",
    "        # thread. Massivley speeds up training because the data is already\n",
    "        # available in the tensorflow graph when the training ops are called.\n",
    "        self.queue = tf.FIFOQueue(\n",
    "            capacity=self.batchsize * 5,\n",
    "            dtypes=[tf.float32, tf.float32, tf.float32, tf.float32, tf.bool],\n",
    "            shapes=[[din], [din], [dout], [], []]\n",
    "        )\n",
    "        self.queue_size = self.queue.size()\n",
    "\n",
    "        # By default we take samples from the queue, but it is also possible\n",
    "        # to directly feed them using a `feed_dict`. The latter is for example\n",
    "        # required when activley using the policy to move in the environment.\n",
    "        x, x_, a, r, t = self.queue.dequeue_many(64)\n",
    "        self.states = tf.placeholder_with_default(x, (None, din), 'states')\n",
    "        self.states_ = tf.placeholder_with_default(x_, (None, din), 'states_')\n",
    "        self.actions = tf.placeholder_with_default(a, (None, dout), 'actions')\n",
    "        self.rewards = tf.placeholder_with_default(r, (None,), 'rewards')\n",
    "        self.terminals = tf.placeholder_with_default(t, (None,), 'terminals')\n",
    "\n",
    "        # This operator will be called in its own thread using the normal\n",
    "        # feed_dict approach to fill the queue with training samples.\n",
    "        self.enqueue_op = self.queue.enqueue_many([\n",
    "            self.states, self.states_, self.actions,\n",
    "            self.rewards, self.terminals,\n",
    "        ])\n",
    "\n",
    "        # Create the online and target actor networks and the noise provider.\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor = make_actor(self.states, dout, self.bounds[1])\n",
    "            actor_ = make_actor(self.states_, dout, self.bounds[1], 'target')\n",
    "            self.noise = noise(dout)\n",
    "\n",
    "        # Create the online and target critic networks. This has a small\n",
    "        # speciallity: The online critic is created twice, once using the\n",
    "        # fed states and fed actions as input and once using the fed states\n",
    "        # and online actor's output as input. The later is required to compute\n",
    "        # the (so called above) `value_gradient`, which directly depends on\n",
    "        # the on-policy instead of the off-policy actions. The important part\n",
    "        # here is that those two critics actually are the same network, just\n",
    "        # with different inputs, but shared (!) parameters.\n",
    "        with tf.variable_scope('critic'):\n",
    "            critic = make_critic(self.states, self.actions)\n",
    "            critic_short = make_critic(self.states, self.actor.y, reuse=True)\n",
    "            critic_ = make_critic(self.states_, actor_.y, 'target')\n",
    "\n",
    "        # Training operations.\n",
    "        with tf.variable_scope('training'):\n",
    "            self.train_op = [\n",
    "                train_critic(critic, critic_, self.terminals, self.rewards),\n",
    "                train_actor(self.actor, critic_short),\n",
    "                soft_updates(critic, critic_),\n",
    "                soft_updates(self.actor, actor_)\n",
    "            ]\n",
    "\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter('logs/lander', self.session.graph)\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "        if checkpoint:\n",
    "            self.saver.restore(self.session, checkpoint)\n",
    "        else:\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            self.session.run(hard_updates(critic, critic_) +\n",
    "                             hard_updates(self.actor, actor_))\n",
    "\n",
    "    def save(self, step):\n",
    "        \"\"\"Save current graph paramter state.\"\"\"\n",
    "        self.saver.save(self.session, 'logs/lander', global_step=step)\n",
    "\n",
    "    def feed_thread(self):\n",
    "        \"\"\"Feed the training queue with data.\"\"\"\n",
    "        while True:\n",
    "            batch = self.memory.sample(self.batchsize)\n",
    "            states, actions, rewards, states_, terminals = zip(*batch)\n",
    "            self.session.run(self.enqueue_op, {\n",
    "                self.states: states, self.actions: actions, self.rewards: rewards,\n",
    "                self.states_: states_, self.terminals: terminals,\n",
    "            })\n",
    "\n",
    "    def train_thread(self):\n",
    "        \"\"\"Train online & soft-update target networks, save summaries.\"\"\"\n",
    "        while True:\n",
    "            self.train_queue.get()  # Only train when allowed to.\n",
    "            summary, global_step, _ = self.session.run(\n",
    "                [self.summaries, self.global_step, self.train_op])\n",
    "            self.writer.add_summary(summary, global_step)\n",
    "            if global_step % 50 == 0:  # Save model from time to time.\n",
    "                self.save(global_step)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Manage feeding and training, allow one more train step.\n",
    "\n",
    "        Starts as many feed and train threads as required in order to\n",
    "        fully take advantage of the available computation resources.\n",
    "        \"\"\"\n",
    "        assert self.memory is not None\n",
    "        if len(self.memory) < self.batchsize:\n",
    "            return\n",
    "\n",
    "        # Start feed threads such that train threads always have data.\n",
    "        if self.session.run(self.queue_size) < (self.batchsize * 2):\n",
    "            thread = Thread(target=self.feed_thread, daemon=True)\n",
    "            thread.start()\n",
    "            self._feed_threads.append(thread)\n",
    "            time.sleep(1)  # Give thread time to start.\n",
    "            print('Feed threads:', len(self._feed_threads))\n",
    "\n",
    "        # Start training threads if there is a training backlog.\n",
    "        if self.train_queue.qsize() > 20:\n",
    "            thread = Thread(target=self.train_thread, daemon=True)\n",
    "            thread.start()\n",
    "            self._train_threads.append(thread)\n",
    "            time.sleep(1)  # Give thread time to start.\n",
    "            print('Train threads:', len(self._train_threads))\n",
    "\n",
    "        # Allow one more training step.\n",
    "        self.train_queue.put(1)\n",
    "\n",
    "    def get_action(self, state, exploration=0):\n",
    "        \"\"\"Map a state to an action according to the current policy.\"\"\"\n",
    "        actions, noise = self.session.run([self.actor.y, self.noise],\n",
    "                                          {self.states: [state]})\n",
    "        action = actions[0] + exploration * noise\n",
    "        action = np.min([action, self.bounds[1]], axis=0)\n",
    "        action = np.max([action, self.bounds[0]], axis=0)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "memory = Memory()\n",
    "model = DDPG(din=env.observation_space.shape[0],\n",
    "             dout=env.action_space.shape[0],\n",
    "             action_bounds=[env.action_space.low, env.action_space.high],\n",
    "             memory=memory)\n",
    "agent = Agent(env, memory, model)\n",
    "agent.train(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'logs/lander-450'\n",
    "\n",
    "# New environment with video recording.\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env = gym.wrappers.Monitor(env, 'logs/lander')\n",
    "\n",
    "# Model and agent from saved state\n",
    "model = DDPG(din=env.observation_space.shape[0],\n",
    "             dout=env.action_space.shape[0],\n",
    "             action_bounds=[env.action_space.low, env.action_space.high],\n",
    "             checkpoint=CHECKPOINT)\n",
    "memory = Memory(model)\n",
    "agent = Agent(env, memory, model, render=False)\n",
    "\n",
    "# Play using learned policy!\n",
    "terminal = False\n",
    "reward = 0\n",
    "state = env.reset()\n",
    "for steps in count():\n",
    "    action = model.get_action(state, 0)\n",
    "    state, r, terminal, _ = env.step(action)\n",
    "    reward += r\n",
    "    env.render()\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "env.close()  # Saves video.\n",
    "\n",
    "print('Epsiode ended after {} steps with a reward of {}'.format(steps, reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
