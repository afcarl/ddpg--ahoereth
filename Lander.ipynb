{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deep Reinforcement Learning on Slot Car Racing\n",
    "\n",
    "In this notebook we provide a quick introduction to the \"Deep Deterministic Gradient Policy\" algorithm presented in the 2016 paper \"Continuous Control with Deep Reinforcement Learning\" by Lillicrap et al. We demonstrate the algorithm in a Slot Car Racing (also known by the brand-name Carrera) environment.\n",
    "\n",
    "The following consists of multiple parts:\n",
    "\n",
    "  - The custom `Carrera` environment -- a Python class which allows agents to create, reset and perform \"steps\" on a slot car racing track. It also provides abstractions for visualizing the algorithms performance.\n",
    "  - An `Agent` class which interacts with a provided environment.\n",
    "  - The DDPG algorithm implemented in TensorFlow.\n",
    "  \n",
    "**TODO:** Fix introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Lunar Lander\n",
    "\n",
    "> Landing pad is always at coordinates $(0, 0)$. Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about $100..140$ points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is $+10$. Firing main engine is $-0.3$ points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from $-1$ to $+1$. First controls main engine, $-1..0$ off, $0..+1$ throttle from $50\\%$ to $100\\%$ power. Engine can't work with less than $50\\%$ power. Second value $-1.0..-0.5$ fire left engine, $+0.5..+1.0$ fire right engine, $-0.5..0.5$ off.\n",
    "\n",
    "In order to run the environments visualizations in a notebook we implemented a small wrapper around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "\n",
    "class Lander:\n",
    "    def __init__(self):\n",
    "        self.gym = gym.make('LunarLanderContinuous-v2')\n",
    "        self.reset = self.gym.reset\n",
    "        self.episode_reward = 0\n",
    "        self._fig = None\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, terminal, info = self.gym.step(action)\n",
    "        self.episode_reward += reward\n",
    "        return state, reward, terminal, info\n",
    "\n",
    "    def render(self):\n",
    "        if self._fig is None:\n",
    "            self._fig = plt.figure()\n",
    "            self._img = plt.imshow(self.gym.render(mode='rgb_array'))\n",
    "        else:\n",
    "            self._img.set_data(self.gym.render(mode='rgb_array'))\n",
    "        self._fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env = Lander()\n",
    "# state = env.reset()\n",
    "# terminal = False\n",
    "# while not terminal:\n",
    "#     action = env.gym.action_space.sample()\n",
    "#     _, _, terminal, _ = env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "While the paper takes note of prioritized replay methods, it only makes use of sampling experiences uniformly from a limited sized buffer. The straight forward approach for implementing this would be to use `collections.deque`, but sampling from such a queue (as the name maybe already shows...) is [expensive](https://wiki.python.org/moin/TimeComplexity). Therefore we implement a custom memory class which makes use of a basic list and implements the element limit through a pointer which dictates which element is to be overwritten on insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Uniform replay memory with maximum size.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self._buffer = []\n",
    "        self._pointer = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self) < self.max_size:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            self._buffer[self._pointer] = experience\n",
    "            self._pointer = (self._pointer + 1) % self.max_size\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self._buffer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Rather abstract implementation of a reinforcement learning agent. The actual RL model is plug and play, as long as it is implemented consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A reinforcement learning agent.\"\"\"\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    batchsize = 64\n",
    "    eps = 1\n",
    "    eps_min = 0\n",
    "    eps_rate = 1 / 100000\n",
    "\n",
    "    def __init__(self, env, model, render=False, memory_size=100000):\n",
    "        \"\"\"Create a new reinforcement learning agent.\"\"\"\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.render = render\n",
    "\n",
    "    def train(self, episodes: int):\n",
    "        \"\"\"Training loop.\"\"\"\n",
    "        stats = []\n",
    "        total_steps = 0\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            for step in count():\n",
    "                # Perform action, store new experience, train model.\n",
    "                action = self._get_action(state)\n",
    "                state_, reward, terminal, _ = self.env.step(action)\n",
    "                self.memory.add((state, action, reward, state_, terminal))\n",
    "                state = state_  # Next state becomes current state.\n",
    "                if self.render:\n",
    "                    self.env.render()\n",
    "                if len(self.memory) >= self.batchsize:\n",
    "                    self.model.train(self.memory.sample(self.batchsize))\n",
    "                if terminal:  # Start new episode if in terminal state.\n",
    "                    stats.append((self.env.episode_reward, step))\n",
    "                    break\n",
    "            total_steps += step\n",
    "            if episode % 100 == 0:\n",
    "                stats = np.asarray(stats)\n",
    "                print('Episode {}, max reward/steps {:.2f}/{:.2f}, average reward/steps {:.2f}/{:.2f}'\n",
    "                      .format(episode, *stats.max(0), *stats.mean(0)))\n",
    "                print('total_steps', total_steps)\n",
    "                stats = []\n",
    "\n",
    "    def _get_action(self, state) -> Tuple[float]:\n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps -= self.eps_rate\n",
    "        return self.model.get_action(state, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Model\n",
    "\n",
    "While we again will create a class for the Deep Deterministic Gradient Policy model, we will implement some of the parts as functions outside of the class in order to better walk through them in this notebook. When implementing this as a script one would want to integrate them all into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import get_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Networks\n",
    "Our model consists of a total of two different networks -- an actor and a critic network. The problem with that approach is that during training we not only optimize those networks, we also use them to dirigate our agent. Manipulating the online policy leads to a feedback loop which leads to instability. While we already use a big memory buffer to mitigate this problem, the authors propose to additionally use two sets of parameters for each network.\n",
    "\n",
    "In the implementation this leads to theoretically four networks, two actor and two critic networks, an online and a target version for each. While the online networks will be used for online predictions and will be updated at every timestep, the target networks will be used for determining the directions in which the online networks should be updated. From time to time the target networks will be updated using the online networks weights -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Networks\n",
    "In order to easily model our networks we use the `namedtuple` collection. Similar to class instances named tuples allow dot-access to their members. For the target networks we only need the network outputs and internal variables, because we will directly manipulate them using the corresponding online network's variables. For the online networks we additionally need a reference to their gradient descent optimizers (for the online training) and direct access to the gradients themself -- more on that in the Actor section.\n",
    "\n",
    "**TODO:** Refresh this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Network = namedtuple('Network', ['y', 'vars', 'ops'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Explain `dense` layer function or get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(x, units, activation, bound=None, decay=None):\n",
    "    if bound is None:\n",
    "        bound = float(x.shape[1].value) ** -.5\n",
    "    if decay is not None:\n",
    "        decay = tf.contrib.layers.l2_regularizer(0.001)\n",
    "    kernel = tf.random_uniform_initializer(-bound, bound)\n",
    "    bias = tf.random_uniform_initializer(-bound, bound)\n",
    "    return tf.layers.dense(x, units, activation=activation,\n",
    "                           kernel_initializer=kernel,\n",
    "                           bias_initializer=bias,\n",
    "                           kernel_regularizer=decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "The critic is the Q value function or Bellman equation approximator. Q values describe the expected reward for an action which normally would be determined through dynamic programming. The critic maps a state/action pair to a single scalar value. This stands in contrast to Deep Q Networks (Mnih et al 2015), where the Q network maps the environment's state to multiple Q values, one for each action. This is because in our case the Q network is not used to determine which action to take, but only to *criticize* whatever action the actor network decided on taking.\n",
    "\n",
    "For the critic network we strickly to the structure described in the paper:\n",
    "\n",
    "  - Two hidden layers with ReLu activation and 400 and 300 neurons respectivley.\n",
    "  - Batch normalization applied to the input and first hidden layer.\n",
    "  - Actions enter the network after the first hidden layer.\n",
    " \n",
    "As common in Deep Q-Networks the single output neuron uses a linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic(x, actions, name='online'):\n",
    "    \"\"\"Build a critic network q, the value function approximator.\"\"\"\n",
    "    in_dim = x.shape[1].value\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        training = tf.shape(x)[0] > 1\n",
    "        norm_0 = tf.layers.batch_normalization(x, training=training)\n",
    "        hidden_1 = dense(norm_0, 400, tf.nn.relu, decay=True)\n",
    "        norm_1 = tf.layers.batch_normalization(hidden_1, training=training)\n",
    "        hidden_1_ = tf.concat([norm_1, actions], axis=1)\n",
    "        hidden_2 = dense(hidden_1_, 300, tf.nn.relu, decay=True)\n",
    "        y = dense(hidden_2, 1, tf.identity, 3e-4, True)\n",
    "        q_values = tf.squeeze(y)\n",
    "        batch_ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        variables = get_variables(scope)\n",
    "    return Network(q_values, variables, batch_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Explain critic optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_critic(critic: Network, qtargets: tf.Tensor):\n",
    "    \"\"\"Build critic network optimizer minimizing MSE.\"\"\"\n",
    "    with tf.variable_scope('critic'):\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        mse = tf.reduce_mean(tf.square(qtargets - critic.y))\n",
    "        with tf.control_dependencies(critic.ops):\n",
    "            return optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor\n",
    "\n",
    "    - Discuss gradient computation/application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor(x, dim_out, name='online'):\n",
    "    \"\"\"Build an actor network mu, the policy function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        training = tf.shape(x)[0] > 1\n",
    "        norm_0 = tf.layers.batch_normalization(x, training=training)\n",
    "        hidden_1 = dense(norm_0, 400, tf.nn.relu)\n",
    "        norm_1 = tf.layers.batch_normalization(hidden_1, training=training)\n",
    "        hidden_2 = dense(hidden_1, 300, tf.nn.relu)\n",
    "        norm_2 = tf.layers.batch_normalization(hidden_2, training=training)\n",
    "        actions = dense(hidden_2, dim_out, tf.nn.tanh, 3e-4)\n",
    "        batch_ops = get_variables(scope, collection=tf.GraphKeys.UPDATE_OPS)\n",
    "        variables = get_variables(scope)\n",
    "    return Network(actions, variables, batch_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Explain actor optimization. Discuss (action/policy) gradient computation/application thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_actor(actor: Network, critic: Network, actions: tf.Tensor):\n",
    "    \"\"\"Build actor network optimizier performing action gradient ascent.\"\"\"\n",
    "    with tf.variable_scope('actor'):\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        action_gradient, = tf.gradients(critic.y, actions)\n",
    "        policy_gradients = tf.gradients(actor.y, actor.vars, -action_gradient)\n",
    "        gradient_pairs = zip(policy_gradients, actor.vars)\n",
    "        with tf.control_dependencies(actor.ops):\n",
    "            return optimizer.apply_gradients(gradient_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network Updates\n",
    "While the online networks are trained directly (thus the *OptimizableNetwork* name), the target networks are only updated irregularily using the online network's parameters. For this paper describes a process named *soft updates*, which only slowly moves the target network's parameters into the direction of the online network. The original Deep Q- and also the Double Deep Q-Network approach instead just directly copies the parameters over.\n",
    "\n",
    "##### Initial Hard Update\n",
    "In order to ensure the online and target networks initial equallity, we first implement the hard parameter copying. This function will only be used after initial variable initialization to make sure the online and target network start off from the same foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hard_updates(src: Network, dst: Network):\n",
    "    \"\"\"Overwrite target with online network parameters.\"\"\"\n",
    "    return [target.assign(online)\n",
    "            for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft Update\n",
    "The soft update also consists of the same assign operation as above, but not directly overwrites the target network's parameters but mashes the online and target parameters together. `tau` herein describes how strongly the new values influence the old values.\n",
    "\n",
    "NOTE: *This could also be implemented using moving averages over the online networks. Might be more efficient?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_updates(src: Network, dst: Network, tau):\n",
    "    \"\"\"Soft update the dst net's parameters using those of the src net.\"\"\"\n",
    "    return [target.assign(tau * online + (1 - tau) * target)\n",
    "            for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise\n",
    "\n",
    "**TODO:** Explain Ornstein-Uhlenbeck process noise and RL exploration strategies.\n",
    "\n",
    "Quote from Lillicrap et al:\n",
    "\n",
    "> For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def noise(n, theta=.15, sigma=.2):\n",
    "    state = tf.Variable(tf.zeros((n,)))\n",
    "    noise = -theta * state + sigma * tf.random_normal((n,))\n",
    "    return state.assign_add(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "  - Initializes networks and session.\n",
    "  - Resets TensorFlow graph because notebooks.\n",
    "  - Copies the initial parameters to the target networks.\n",
    "  - Provides `train` function which counts SGD steps.\n",
    "  - Target networks are updated every n SGD steps.\n",
    "  - Provides `get_action`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\"Deep Deterministic Policy Gradient RL Model.\"\"\"\n",
    "    gamma = 0.99  # Discount factor\n",
    "    theta = 0.15  # Ornstein-Uhlenbeck process theta\n",
    "    mu = 0.2  # Ornstein-Uhlenbeck process mu\n",
    "\n",
    "    def __init__(self, dim_in=2, dim_out=1, tau=1e-3):\n",
    "        \"\"\"Create a new DDPG model.\"\"\"\n",
    "        tf.reset_default_graph()  # Graph might contain nodes from last run.\n",
    "\n",
    "        self.states = tf.placeholder(tf.float32, (None, dim_in), 'inputs')\n",
    "        self.actions = tf.placeholder(tf.float32, (None, dim_out), 'actions')\n",
    "        self.qtargets = tf.placeholder(tf.float32, (None,), 'qtargets')\n",
    "        self.rewards = tf.placeholder(tf.float32, (None,), 'rewards')\n",
    "        self.terminals = tf.placeholder(tf.bool, (None,), 'terminals')\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor = actor(self.states, dim_out)\n",
    "            self.actor_ = actor(self.states, dim_out, 'target')\n",
    "            self.noise = noise(dim_out, self.theta, self.mu)\n",
    "\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic = critic(self.states, self.actions)\n",
    "            self.critic_ = critic(self.states, self.actions, 'target')\n",
    "\n",
    "        with tf.variable_scope('training'):\n",
    "            self.critic_op = train_critic(self.critic, self.qtargets)\n",
    "            self.actor_op = train_actor(self.actor, self.critic, self.actions)\n",
    "            self.targets_op = (soft_updates(self.critic, self.critic_, tau) +\n",
    "                               soft_updates(self.actor, self.actor_, tau))\n",
    "\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.session.run(hard_updates(self.critic, self.critic_) +\n",
    "                         hard_updates(self.actor, self.actor_))\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"Train the online and update the target networks.\"\"\"\n",
    "        states, actions, rewards, states_, terminals = zip(*batch)\n",
    "\n",
    "        # Train critic.\n",
    "        # 1. Get expected maximum rewards for next state from target critic.\n",
    "        # 2. Approximate bellman/dynamic programming equation.\n",
    "        # 3. Update critic network minimizing MSE.\n",
    "        qvalues = self.session.run(self.critic_.y, {self.states: states_,\n",
    "                                                    self.actions: actions})\n",
    "        bellman = rewards + self.gamma * qvalues * np.invert(terminals)\n",
    "        self.session.run(self.critic_op, {self.states: states,\n",
    "                                          self.actions: actions,\n",
    "                                          self.qtargets: bellman})\n",
    "\n",
    "        # Train actor.\n",
    "        # 1. Get most up-to-date actions for current state from online actor.\n",
    "        # 2. Update actor network using policy gradient.\n",
    "        actions_ = self.session.run(self.actor.y, {self.states: states})\n",
    "        self.session.run(self.actor_op, {self.states: states,\n",
    "                                         self.actions: actions_})\n",
    "\n",
    "        # Update target networks using soft updates.\n",
    "        self.session.run(self.targets_op)\n",
    "\n",
    "    def get_action(self, state, exploration=0):\n",
    "        actions, noise = self.session.run([self.actor.y, self.noise],\n",
    "                                          {self.states: [state]})\n",
    "        action = actions[0] + exploration * noise\n",
    "        action[action < -1] = -1\n",
    "        action[action > 1] = 1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Lander()\n",
    "print(env.gym.action_space.shape,\n",
    "      env.gym.action_space.low,\n",
    "      env.gym.action_space.high)\n",
    "model = DDPG(dim_in=8, dim_out=2)\n",
    "agent = Agent(env, model, render=True)\n",
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Lander()\n",
    "terminal = False\n",
    "state = env.reset()\n",
    "env.render()\n",
    "while not terminal:\n",
    "    action = model.get_action(state, False)\n",
    "    state, _, terminal, _ = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
