{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Deep Reinforcement Learning\n",
    "\n",
    "In this notebook we provide a quick introduction to the \"Deep Deterministic Gradient Policy\" algorithm presented in the 2016 paper \"Continuous Control with Deep Reinforcement Learning\" by Lillicrap et al. We demonstrate the algorithm in a Slot Car Racing (also known by the brand-name Carrera) environment.\n",
    "\n",
    "The following consists of multiple parts:\n",
    "  \n",
    "**TODO:** Fix introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: Lunar Lander\n",
    "\n",
    "> Landing pad is always at coordinates $(0, 0)$. Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about $100..140$ points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is $+10$. Firing main engine is $-0.3$ points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from $-1$ to $+1$. First controls main engine, $-1..0$ off, $0..+1$ throttle from $50\\%$ to $100\\%$ power. Engine can't work with less than $50\\%$ power. Second value $-1.0..-0.5$ fire left engine, $+0.5..+1.0$ fire right engine, $-0.5..0.5$ off.\n",
    "\n",
    "Note: Rendering does not work when running on a remote machine or similar. We actually had [rendering directly into notebooks](https://github.com/ahoereth/ddpg/blob/f2fcad4/Lander.ipynb) implemented, but were not able to get it to work when using docker for training using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "state = env.reset()\n",
    "terminal = False\n",
    "while not terminal:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, terminal, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "While the paper takes note of prioritized replay methods, it only makes use of sampling experiences uniformly from a limited sized buffer. The straight forward approach for implementing this would be to use `collections.deque`, but sampling from such a queue (as the name maybe already shows...) is [expensive](https://wiki.python.org/moin/TimeComplexity). Therefore we implement a custom memory class which makes use of a basic list and implements the element limit through a pointer which dictates which element is to be overwritten on insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Uniform replay memory with maximum size.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self._buffer = []\n",
    "        self._pointer = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffer)\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self) < self.max_size:\n",
    "            self._buffer.append(experience)\n",
    "        else:\n",
    "            self._buffer[self._pointer] = experience\n",
    "            self._pointer = (self._pointer + 1) % self.max_size\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self._buffer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "Rather abstract implementation of a reinforcement learning agent. The actual RL model is plug and play, as long as it is implemented consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from typing import Tuple\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"A reinforcement learning agent.\"\"\"\n",
    "    theta = 0.15\n",
    "    sigma = 0.2\n",
    "    batchsize = 64\n",
    "    eps = 1\n",
    "    eps_min = .1\n",
    "    feed_threads = 1\n",
    "\n",
    "    def __init__(self, env, model, render=False, memory_size=100000):\n",
    "        \"\"\"Create a new reinforcement learning agent.\"\"\"\n",
    "        self.memory = Memory(memory_size)\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.render = render\n",
    "        self.feeding = False\n",
    "        self.feeders = [Thread(target=self.feed, daemon=True)\n",
    "                        for _ in range(self.feed_threads)]\n",
    "\n",
    "    def feed(self):\n",
    "        \"\"\"Feed samples from memory to the model for training.\n",
    "\n",
    "        The model's feed() function blocks when it does not need more data,\n",
    "        so we can aggressivley loop here.\n",
    "        \"\"\"\n",
    "        while self.feeding:\n",
    "            self.model.feed(self.memory.sample(self.batchsize))\n",
    "\n",
    "    def train(self, episodes: int):\n",
    "        \"\"\"Play many episodes with many steps.\n",
    "\n",
    "        Stores observations in memory, starts the feed threads and calls\n",
    "        the model's training function when memory is big enough.\n",
    "        \"\"\"\n",
    "        eps_rate = 1 / (episodes / 2)\n",
    "        stats = []\n",
    "        total_steps = 0\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in count():\n",
    "                # Perform action, store new experience, train model.\n",
    "                action = self._get_action(state)\n",
    "                state_, reward, terminal, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                self.memory.add((state, action, reward, state_, terminal))\n",
    "                state = state_  # Next state becomes current state.\n",
    "                if self.render:\n",
    "                    self.env.render()\n",
    "                if len(self.memory) >= self.batchsize:\n",
    "                    if not self.feeding:\n",
    "                        self.feeding = True\n",
    "                        for feeder in self.feeders:\n",
    "                            feeder.start()\n",
    "                    self.model.train()\n",
    "                if terminal:  # Start new episode if in terminal state.\n",
    "                    stats.append((episode_reward, step))\n",
    "                    break\n",
    "            total_steps += step\n",
    "            if self.eps > self.eps_min:\n",
    "                self.eps -= eps_rate\n",
    "            if episode % 50 == 0 or episode == episodes:\n",
    "                self.model.save(episode)\n",
    "            if episode % 10 == 0:\n",
    "                stats = np.asarray(stats)\n",
    "                rargmax, _ = stats.argmax(0)\n",
    "                print('Episode {}, max reward/steps {:.2f}/{:.2f}, '\n",
    "                      'average reward/steps {:.2f}/{:.2f}'\n",
    "                      .format(episode, *stats[rargmax], *stats.mean(0)))\n",
    "                print('total_steps', total_steps)\n",
    "                stats = []\n",
    "\n",
    "    def _get_action(self, state) -> Tuple[float]:\n",
    "        \"\"\"Get a action vector from the model.\"\"\"\n",
    "        return self.model.get_action(state, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Model\n",
    "\n",
    "While we again will create a class for the Deep Deterministic Gradient Policy model, we will implement some of the parts as functions outside of the class in order to better walk through them in this notebook. When implementing this as a script one would want to integrate them all into the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import get_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor and Critic Networks\n",
    "Our model consists of a total of two different networks -- an actor and a critic network. The problem with that approach is that during training we not only optimize those networks, we also use them to dirigate our agent. Manipulating the online policy leads to a feedback loop which leads to instability. While we already use a big memory buffer to mitigate this problem, the authors propose to additionally use two sets of parameters for each network.\n",
    "\n",
    "In the implementation this leads to theoretically four networks, two actor and two critic networks, an online and a target version for each. While the online networks will be used for online predictions and will be updated at every timestep, the target networks will be used for determining the directions in which the online networks should be updated. From time to time the target networks will be updated using the online networks weights -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract Networks\n",
    "In order to easily model all the networks which we need to create, we use the `namedtuple` collection. Similar to class instances named tuples allow dot-access to their members. Each network has some output `y` and some variables `var`. The advantage of such an approach is that members of the graph are more consistently accessible and additional features (for example `operations` for updating batch normalization running averages) can be added later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Network = namedtuple('Network', ['y', 'vars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layers\n",
    "Lillicrap et al 2016 describes very specific dense layers:\n",
    "\n",
    "- Weights and biases are initialized from a bounded uniform distribution where the bounds depend on the previous layer's size (or are fixed for output layers).\n",
    "- All critic layers use l2 weight decay.\n",
    "\n",
    "In order to not repeat all the initialization code we predefine a wrapper function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, units, activation=tf.identity, decay=None, minmax=None):\n",
    "    \"\"\"Build a dense layer with uniform initialization and optional loss.\"\"\"\n",
    "    if minmax is None:\n",
    "        minmax = float(x.shape[1].value) ** -.5\n",
    "    return tf.layers.dense(\n",
    "        x,\n",
    "        units,\n",
    "        activation=activation,\n",
    "        kernel_initializer=tf.random_uniform_initializer(-minmax, minmax),\n",
    "        bias_initializer=tf.random_uniform_initializer(-minmax, minmax),\n",
    "        kernel_regularizer=decay and tf.contrib.layers.l2_regularizer(1e-3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critic\n",
    "The critic is the value function, you can also think about this as the Bellman equation approximator. It returns Q values which describe the expected reward for an action. Those values would normally be determined through dynamic programming. The critic maps a state/action pair to a single scalar value. This stands in contrast to Deep Q Networks (Mnih et al 2015), where the Q network maps the environment's state to a vector of Q values, one for each action. This is because in our case the Q network is not used to determine which action to take (DQN uses a deterministic `argmax` policy on the Q value vector), but only to *criticize* whatever action the actor network decides on taking.\n",
    "\n",
    "We strictly stick to the network structure described in the paper:\n",
    "\n",
    "  - Two hidden layers with ReLu activation and 400 and 300 neurons respectivley.\n",
    "  - <s>Batch normalization applied to the input and first hidden layer.</s><br>*[Removed](https://github.com/ahoereth/ddpg/blob/https://github.com/ahoereth/ddpg/blob/ea02a94/Lander.ipynb) batch normalization  because it let to massive instability.*\n",
    "  - Actions enter the network after the first hidden layer.\n",
    " \n",
    "As common in Deep Q-Networks the single output neuron uses a linear activation. The `critic` function below is designed such that we can reuse it for both the online and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic(x: tf.Tensor, actions: tf.Tensor, name='online', reuse=False):\n",
    "    \"\"\"Build a critic network q, the value function approximator.\"\"\"\n",
    "    with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "        net = dense(x, 400, tf.nn.relu, decay=True)\n",
    "        net = tf.concat([net, actions], axis=1)\n",
    "        net = dense(net, 300, tf.nn.relu, decay=True)\n",
    "        y = dense(net, 1, decay=True, minmax=3e-4)\n",
    "        return Network(tf.squeeze(y), get_variables(scope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critic is simply optimized by minimizing the mean squared error between the Q target values which are computed using the Bellman approximation on the experiences we make in the environment (we will do so below) and the actual output from the network. In the following cell we also bind the operations of the online critic (to update the running averages) to the minimize operation -- this way they will be called everytime we call the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(critic: Network, critic_: tf.Tensor, terminals: tf.Tensor, \n",
    "                 rewards: tf.Tensor, gamma: float):\n",
    "    \"\"\"Build critic network optimizer minimizing MSE.\"\"\"\n",
    "    with tf.variable_scope('critic'):\n",
    "        targets = tf.where(terminals, rewards, rewards + gamma * critic_.y)\n",
    "        mse = tf.reduce_mean(tf.squared_difference(targets, critic.y))\n",
    "        tf.summary.scalar('loss', mse)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        return optimizer.minimize(mse, tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor\n",
    "Actor networks describe the policy the agent should follow in any given state. Given some input the policy deterministically provides an action. Actions here are vectors of real values -- in a racing environment such a vector would for example contain a steering angle and a acceleration value.\n",
    "\n",
    "The actor networks have a simillar structure as the critic, but do not receive the actions as input at any point. Further more there is no weight decay applied and the output uses a hyperbolic tangent activation function to bound the actions between $-1$ and $1$ -- which needs to be tweaked for different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(x: tf.Tensor, dout: int, max_out: float, name='online'):\n",
    "    \"\"\"Build an actor network mu, the policy function approximator.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        net = dense(x, 400, tf.nn.relu)\n",
    "        net = dense(net, 300, tf.nn.relu)\n",
    "        y = dense(net, dout, tf.nn.tanh, minmax=3e-4)\n",
    "        scaled = y * max_out\n",
    "        return Network(scaled, get_variables(scope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Gradients\n",
    "**TODO**: Explain actor optimization. Discuss (action/policy) gradient computation/application thoroughly.\n",
    "\n",
    "Our goal is to tweak the critic network's parameters such that it outputs action values, which, as input to the critic network, result in good results (aka high Q values aka profit). \n",
    "\n",
    "The actor is trained by ascending the gradients of the critic with respect to the actor's actions.\n",
    "\n",
    "To do so, we need to perform gradient **ascent** on the critic's gradient with respect to the actor's action output (which is an input to the critic, a necessary condition for computing such gradient). That gradient, lets call it *value gradient*, depicts the direction in which to adjust the action (originally the output of the actor network) to minimize the Q values -- we ascent it, because we want to maximize the Q values.\n",
    "$$value\\_gradient = \\Delta_a Q(s,a|\\theta^Q)|_{s=s_i, a=\\mu(s_i)}$$\n",
    "\n",
    "The next step is to take those gradients to the **actor** network, although they originally came from the **critic** network. Normally TensorFlow would worry about optimizing the network for us by minimizing some scalar loss we provide. The magic in this is, that one does not actually pass a scalar to the optimizer's `minimize` function, but a Tensor node, which depends on a complete graph of computations -- those are just commonly abstracted away. When aiming to minimize such a loss, TensorFlow visits all the nodes on which that final loss node depends and internally computes the gradients to modulate them a little bit such that the loss decays.\n",
    "\n",
    "Our problem is: We do not have such a loss node, but a complete set of gradients from the critic network. We now need to manually modulate the actor network's gradients, such that they move a little bit into the uphill direction of the critic network's gradients.\n",
    "\n",
    "$$policy\\_gradient = value\\_gradient \\Delta_{\\theta, \\mu} \\mu(s|\\theta^\\mu)|_{s_i}$$\n",
    "\n",
    "\n",
    "$$\\Delta_{\\theta, \\mu}J \\approx \\frac{1}{N} \\sum_i \\Delta_a Q(s,a|\\theta^Q)|_{s=s_i, a=\\mu(s_i)} \\Delta_{\\theta^\\mu} \\mu(s|\\theta^\\mu)|_{s_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(actor: Network, critique: tf.Tensor):\n",
    "    \"\"\"Build actor network optimizier performing action gradient ascent.\"\"\"\n",
    "    with tf.variable_scope('actor'):\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "\n",
    "        # What is `actor.y`'s influence on the critic network's output?\n",
    "        value_gradient, = tf.gradients(critique, actor.y)\n",
    "\n",
    "        # Use `value_gradient` as initial value for the `actor.y` gradients --\n",
    "        # normally this is set to 1s by TF.\n",
    "        policy_gradients = tf.gradients(actor.y, actor.vars, -value_gradient)\n",
    "        gradient_pairs = zip(policy_gradients, actor.vars)\n",
    "        return optimizer.apply_gradients(gradient_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Network Updates\n",
    "While the online networks are trained directly (thus the *OptimizableNetwork* name), the target networks are only updated irregularily using the online network's parameters. For this paper describes a process named *soft updates*, which only slowly moves the target network's parameters into the direction of the online network. The original Deep Q- and also the Double Deep Q-Network approach instead just directly copies the parameters over.\n",
    "\n",
    "##### Initial Hard Update\n",
    "In order to ensure the online and target networks initial equallity, we first implement the hard parameter copying. This function will only be used after initial variable initialization to make sure the online and target network start off from the same foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_updates(src: Network, dst: Network):\n",
    "    \"\"\"Overwrite target with online network parameters.\"\"\"\n",
    "    with tf.variable_scope('hardupdates'):\n",
    "        return [target.assign(online)\n",
    "                for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Soft Update\n",
    "The soft update also consists of the same assign operation as above, but not directly overwrites the target network's parameters but mashes the online and target parameters together. `tau` herein describes how strongly the new values influence the old values.\n",
    "\n",
    "NOTE: *This could also be implemented using moving averages over the online networks. Might be more efficient?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_updates(src: Network, dst: Network, tau=1e-3):\n",
    "    \"\"\"Soft update the dst net's parameters using those of the src net.\"\"\"\n",
    "    with tf.variable_scope('softupdates'):\n",
    "        return [target.assign(tau * online + (1 - tau) * target)\n",
    "                for online, target in zip(src.vars, dst.vars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise\n",
    "\n",
    "**TODO:** Explain Ornstein-Uhlenbeck process noise and RL exploration strategies.\n",
    "\n",
    "Quote from Lillicrap et al:\n",
    "\n",
    "> For the exploration noise process we used temporally correlated noise in order to explore well in physical environments that have momentum. We used an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) with θ = 0.15 and σ = 0.2. The Ornstein-Uhlenbeck process models the velocity of a Brownian particle with friction, which results in temporally correlated values centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n, theta=.15, sigma=.2):\n",
    "    with tf.variable_scope('OUNoise'):\n",
    "        state = tf.Variable(tf.zeros((n,)))\n",
    "        noise = -theta * state + sigma * tf.random_normal((n,))\n",
    "        reset = state.assign(tf.zeros((n,)))\n",
    "        return state.assign_add(noise), reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "  - Initializes networks and session.\n",
    "  - Resets TensorFlow graph because notebooks.\n",
    "  - Copies the initial parameters to the target networks.\n",
    "  - Provides `train` function which counts SGD steps.\n",
    "  - Target networks are updated every n SGD steps.\n",
    "  - Provides `get_action`.\n",
    "  \n",
    "**TODO:** The fancy input pipeline using a queue is great, but can the code for it be simplified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\"Deep Deterministic Policy Gradient RL Model.\"\"\"\n",
    "    gamma = 0.99  # Discount factor\n",
    "    theta = 0.15  # Ornstein-Uhlenbeck process theta\n",
    "    mu = 0.2  # Ornstein-Uhlenbeck process mu\n",
    "\n",
    "    def __init__(self, din=2, dout=1, action_bounds=1, checkpoint=None):\n",
    "        \"\"\"Create a new DDPG model.\"\"\"\n",
    "        self.bounds = action_bounds\n",
    "\n",
    "        # Reset graph and recreate it.\n",
    "        tf.reset_default_graph()\n",
    "        tf.train.create_global_step()\n",
    "        self.global_step = tf.train.get_global_step()\n",
    "        self.session = tf.Session()\n",
    "\n",
    "        # The queue is fed samples from the replay memory in an independent\n",
    "        # thread. Massivley speeds up training because the data is already\n",
    "        # available in the tensorflow graph when the training ops are called.\n",
    "        self.queue = tf.FIFOQueue(\n",
    "            capacity=64 * 3,\n",
    "            dtypes=[tf.float32, tf.float32, tf.float32, tf.float32, tf.bool],\n",
    "            shapes=[[din], [din], [dout], [], []]\n",
    "        )\n",
    "\n",
    "        # By default we take samples from the queue, but it is also possible\n",
    "        # to directly feed them using a `feed_dict`. The latter is for example\n",
    "        # required when activley using the policy to move in the environment.\n",
    "        x, x_, a, r, t = self.queue.dequeue_many(64)\n",
    "        self.states = tf.placeholder_with_default(x, (None, din), 'states')\n",
    "        self.states_ = tf.placeholder_with_default(x_, (None, din), 'states_')\n",
    "        self.actions = tf.placeholder_with_default(a, (None, dout), 'actions')\n",
    "        self.rewards = tf.placeholder_with_default(r, (None,), 'rewards')\n",
    "        self.terminals = tf.placeholder_with_default(t, (None,), 'terminals')\n",
    "\n",
    "        # This operator will be called in its own thread using the normal\n",
    "        # feed_dict approach to fill the queue with training samples.\n",
    "        self.enqueue_op = self.queue.enqueue_many([\n",
    "            self.states, self.states_, self.actions,\n",
    "            self.rewards, self.terminals,\n",
    "        ])\n",
    "\n",
    "        # Create the online and target actor networks and the noise provider.\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor = actor(self.states, dout, self.bounds[1])\n",
    "            self.actor_ = actor(self.states_, dout, self.bounds[1], 'target')\n",
    "            self.noise, self.noise_reset = noise(dout, self.theta, self.mu)\n",
    "\n",
    "        # Create the online and target critic networks. This has a small\n",
    "        # speciallity: The online critic is created twice, once using the\n",
    "        # fed states and fed actions as input and once using the fed states\n",
    "        # and online actor's output as input. The later is required to compute\n",
    "        # the (so called above) `value_gradient`, which directly depends on\n",
    "        # the on-policy instead of the off-policy actions. The important part\n",
    "        # here is that those two critics actually are the same network, just\n",
    "        # with different inputs, but shared (!) parameters.\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic = critic(self.states, self.actions)\n",
    "            critique, _ = critic(self.states, self.actor.y, reuse=True)\n",
    "            self.critic_ = critic(self.states_, self.actor_.y, 'target')\n",
    "\n",
    "        with tf.variable_scope('training'):\n",
    "            self.critic_op = train_critic(self.critic, self.critic_,\n",
    "                                          self.terminals, self.rewards,\n",
    "                                          self.gamma)\n",
    "            self.actor_op = train_actor(self.actor, critique)\n",
    "            self.targets_op = (soft_updates(self.critic, self.critic_) +\n",
    "                               soft_updates(self.actor, self.actor_))\n",
    "\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        self.writer = tf.summary.FileWriter('logs/lander', self.session.graph)\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "        if checkpoint:\n",
    "            self.saver.restore(self.session, checkpoint)\n",
    "        else:\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            self.session.run(hard_updates(self.critic, self.critic_) +\n",
    "                             hard_updates(self.actor, self.actor_))\n",
    "\n",
    "    def feed(self, batch):\n",
    "        \"\"\"Feed the training queue with data.\"\"\"\n",
    "        states, actions, rewards, states_, terminals = zip(*batch)\n",
    "        self.session.run(self.enqueue_op, {\n",
    "            self.states: states, self.actions: actions, self.rewards: rewards,\n",
    "            self.states_: states_, self.terminals: terminals,\n",
    "        })\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset the OU process exploration noise.\n",
    "        \n",
    "        Note: Currently unused, unsure if it a fresh OU process would provide\n",
    "        any advantage.\n",
    "        \"\"\"\n",
    "        self.session.run(self.noise_reset)\n",
    "\n",
    "    def save(self, step):\n",
    "        \"\"\"Save current graph paramter state.\"\"\"\n",
    "        self.saver.save(self.session, 'logs/lander', global_step=step)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the online and update the target networks. Save summaries.\"\"\"\n",
    "        summary, _, _, _, i = self.session.run([self.summaries, self.critic_op,\n",
    "                                                self.actor_op, self.targets_op,\n",
    "                                                self.global_step])\n",
    "        self.writer.add_summary(summary, i)\n",
    "\n",
    "    def get_action(self, state, exploration=0):\n",
    "        \"\"\"Map a state to an action according to the current policy.\n",
    "        \n",
    "        TODO: Consider moving noise application and action bounding to TF?\n",
    "        \"\"\"\n",
    "        actions, noise = self.session.run([self.actor.y, self.noise],\n",
    "                                          {self.states: [state]})\n",
    "        action = actions[0] + exploration * noise\n",
    "        action = np.min([action, self.bounds[1]], axis=0)\n",
    "        action = np.max([action, self.bounds[0]], axis=0)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "# env = gym.make('Pendulum-v0')\n",
    "model = DDPG(din=env.observation_space.shape[0], \n",
    "             dout=env.action_space.shape[0], \n",
    "             action_bounds=[env.action_space.low, env.action_space.high])\n",
    "agent = Agent(env, model, render=False)\n",
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay from saved checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = 'logs/lander-950'\n",
    "\n",
    "# New environment with video recording.\n",
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env = gym.wrappers.Monitor(env, 'logs/lander')\n",
    "\n",
    "# Model and agent from saved state\n",
    "model = DDPG(din=env.observation_space.shape[0], \n",
    "             dout=env.action_space.shape[0], \n",
    "             action_bounds=[env.action_space.low, env.action_space.high],\n",
    "             checkpoint=CHECKPOINT)\n",
    "agent = Agent(env, model, render=False)\n",
    "\n",
    "# Play using learned policy!\n",
    "terminal = False\n",
    "reward = 0\n",
    "state = env.reset()\n",
    "for steps in  count():\n",
    "    action = model.get_action(state, 0)\n",
    "    state, r, terminal, _ = env.step(action)\n",
    "    reward += r\n",
    "    env.render()\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "env.close()  # Saves video.\n",
    "\n",
    "print('Epsiode ended after {} steps with a reward of {}'.format(steps, reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddpg",
   "language": "python",
   "name": "ddpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
